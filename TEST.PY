# âœ…todo : 1. colab ìš©ìœ¼ë¡œ requirements.txtì„ í•˜ë‚˜ ë” ë§Œë“ ë‹¤.
# âœ… todo : 1. turnonselenium ë‚´ìš©ì„ colabìš©ìœ¼ë¡œ ì¶”ê°€í•œë‹¤.
# todo : 1. bloggerë¥¼ ëª¨ë“ˆí™”í•´ì„œ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ ë§Œë“ ë‹¤.
# âœ…todo : 1. ì“°ë ˆë”© ì‹œí‚¤ëŠ” ê²ƒë„ í•¨ìˆ˜í™”í•œë‹¤.
# âœ…todo : 1. ì½”ë©ì€ ì“°ë ˆë”© ëª‡ê°œë¥¼ ì‹œí‚¤ëŠ”ê²Œ ì ë‹¹í•œì§€ë¥¼ í™•ì¸í•œë‹¤ = 2ê°œ (2ì½”ì–´ë¼ì„œ)
# âœ…todo : 1. ì½”ë©í¬ë¡¤ë§ ì‹œì—, user-agent ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤.
# duckduckgo ëŒ€ì²´í’ˆ ì°¾ê¸°



import os
# ì •ì˜í•œ í´ë˜ìŠ¤ë“¤ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
from module.Crawler import Crawler
from module.AIAgent import AIAgent
from module.File_manager import File_manager
from module.Uploader import Uploader
import copy

class Blogger() :
    def __init__(self, blogname, verbose = False, isHeadless = True) :         
        self.blogname = blogname # ë¸”ë¡œê·¸ ì´ë¦„
        self.verbose = verbose # ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """ ë°˜ë“œì‹œ íŒŒì¼ë§¤ë‹ˆì €ë¶€í„° í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤. íŒŒì¼ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ê³¼ í•¨ê»˜ ëª¨ë“  í´ë”êµ¬ì¡°ê°€ ìƒì„±ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."""
        # í´ë˜ìŠ¤ í˜¸ì¶œ        
        self.file_manager = File_manager(blogname = blogname, verbose=verbose) # íŒŒì¼ ë§¤ë‹ˆì €
        self.crawler = Crawler(verbose=verbose, isHeadless=isHeadless) # í¬ë¡¤ëŸ¬
        self.posting_ai = AIAgent(index="my_contents", blogname=blogname  ,verbose=verbose) # í¬ìŠ¤íŒ… AI
        self.keyword_ai = AIAgent(index = "my_keywords", blogname=blogname, verbose=verbose) # í‚¤ì›Œë“œ AI
        self.uploader = Uploader(
            blogname=blogname, # ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸, í¬ìŠ¤íŒ…URLì„ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.        
            verbose=verbose,
            isHeadless = isHeadless
            )
        
        # ê¸°ì¡´ì— ìˆ˜ì§‘í•˜ì—¬ì„œ ì €ì¥í•´ë‘ì—ˆë˜ í‚¤ì›Œë“œë“¤ì´ ìˆìœ¼ë©´ì€ ë¶ˆëŸ¬ì™€ì„œ crawler.resultsì— ì €ì¥í•©ë‹ˆë‹¤.
        # ê·¸ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê³¼ì •ì—ì„œ ì¤‘ë³µë˜ëŠ” í‚¤ì›Œë“œë¥¼ ì œê±°í•  ìˆ˜ ìˆì„ ë¿ë§Œ ì•„ë‹ˆë¼ ìƒˆë¡­ê²Œ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œê°€ ê¸°ì¡´ í‚¤ì›Œë“œì— ë®ì–´ì”Œì›Œì§€ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.

        collected_keywords_info = self.file_manager.get_keywords(what_keywords='collected_keywords') # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´            
        if collected_keywords_info is not None : # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ê°€ ìˆìœ¼ë©´
            self.crawler.results = copy.deepcopy(collected_keywords_info.T.to_dict()) # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ë¥¼ crawler.resultsì— ì €ì¥í•©ë‹ˆë‹¤.
        else :            
            print('í•´ë‹¹ ë¸”ë¡œê·¸ëŠ” ìˆ˜ì§‘í•œ í‚¤ì›Œë“œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')        
        
        # self.collected_keywords_info = self.file_manager.get_keywords(what_keywords='collected_keywords')
        # self.collected_keywords = self.collected_keywords_info.keys().to_list()
        # self.screened_keywords_info = self.file_manager.get_keywords(what_keywords='collected_keywords')
        # self.screened_keywords = self.screened_keywords_info.keys().to_list()
        # self.suitable_keywords_info = self.file_manager.get_keywords(what_keywords='collected_keywords')
        # self.suitable_keywords = self.suitable_keywords_info.keys().to_list()

        


    def collect_keywords(self, subjects_n_words, depth, save=True) :
        self.crawler.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        try :
            # í‚¤ì›Œë“œ ìˆ˜ì§‘ì—ëŠ” crawler í´ë˜ìŠ¤ ë‚´ì—ì„œ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆëŠ” ë°˜ë³µë¬¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            self.crawler.iterate_keyword_crawling_w_multiple_subjects(depth = depth, subjects_n_words=subjects_n_words, save=save) 
        except Exception as e :
            print(e)
        finally : # ì¤‘ë‹¨í•˜ë”ë¼ë„ í˜„ì¬ì§„í–‰ì‹œì ê¹Œì§€ê°€ ë¡œì»¬ì— ì €ì¥ë©ë‹ˆë‹¤.
            
            # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ ì¤‘ì—ì„œ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ í‚¤ì›Œë“œëŠ” ì œì™¸í•˜ê³  ì €ì¥í•œë‹¤.
            # ê·¸ëŸ°ë° ì´ ê¸°ëŠ¥ì€ ì—¬ê¸°ì— ë“¤ì–´ê°ˆ ê²ƒì´ ì•„ë‹ˆë¼ ê¸°ë³¸ì ì¸ Cralwerì˜ iteration í•¨ìˆ˜ì— ë“¤ì–´ê°€ì•¼ í•œë‹¤.
            self.crawler.results =  self.crawler.load_results().dropna(
                subset = ['num_ads', 'tistory_rank_at_google']).T.to_dict() 
            print('after interuption : good')

            # collected_keywords
            collected_keywords_info = self.crawler.load_results() # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´
            collected_keywords = self.crawler.get_keywords() # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            self.file_manager.save_keywords('collected_keywords', self.crawler.results) # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ ì €ì¥
            print('after interuption : collected_keywords = good')


            # í‚¤ì›Œë“œ ì„ ë³„
            screened_keywords_info = self.crawler.load_processed_results() # ì„ ë³„í•œ í‚¤ì›Œë“œ ì •ë³´
            screened_keywords = list(screened_keywords_info.index) # ì„ ë³„í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            self.file_manager.save_keywords('screened_keywords',  self.crawler.results ) # ì„ ë³„í•œ í‚¤ì›Œë“œ ì •ë³´ ì €ì¥
            print('after interuption : screened_keywords = good')

            # í‚¤ì›Œë“œ ì ì •ì„± ê²€ì‚¬
            subjects = screened_keywords_info.loc[:,'subject'].to_list() # í‚¤ì›Œë“œ ì ì •ì„± ê²€ì‚¬ë¥¼ ìœ„í•œ ì£¼ì œ ë¦¬ìŠ¤íŠ¸            
            suitable_keywords = self.keyword_ai.suitability_checker(subjects, screened_keywords) # ì ì •ì„± ê²€ì‚¬
            suitable_keywords_info = screened_keywords_info.loc[suitable_keywords] # ì ì •ì„± ê²€ì‚¬ ê²°ê³¼
            print('after interuption : good3')
            
            # í´ë˜ìŠ¤ ë‚´ë¶€ ë³€ìˆ˜ë¡œ ì €ì¥
            self.collected_keywords_info = collected_keywords_info # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´
            self.collected_keywords = collected_keywords # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            self.screened_keywords_info = screened_keywords_info # ì„ ë³„í•œ í‚¤ì›Œë“œ ì •ë³´
            self.screened_keywords = screened_keywords# ì„ ë³„í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            self.suitable_keywords_info = suitable_keywords_info # ì ì •í•œ í‚¤ì›Œë“œ ì •ë³´
            self.suitable_keywords = suitable_keywords # ì ì •í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            print('after interuption : good4')
            
            # ìˆ˜ì§‘í•œ ì „ì²´ í‚¤ì›Œë“œ ë¡œì»¬í™˜ê²½ì— csvíŒŒì¼ë¡œ ì €ì¥            
            
            self.file_manager.save_keywords('suitable_keywords', suitable_keywords_info.T.to_dict() ) # ì ì •í•œ í‚¤ì›Œë“œ ì •ë³´ ì €ì¥
            
            
            # í‚¤ì›Œë“œ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ (keyword_ai)            
            self.keyword_ai.vectorstore_save_texts(suitable_keywords)

    def create_contents(self, 
                        num_contents_creation = None, 
                        keyword_score_threshold=0.55,
                        kewyord_retriever_k = 16,
                        num_topic = 3,
                        contents_lan = 'English',
                        num_images = 1,
                        documents_score_threshold=0.25,
                        documents_retriever_k = 10) :        

        iteration = 0 # ë°˜ë³µíšŸìˆ˜
        # ë¬´í•œë°˜ë³µ (ë°˜ë³µíšŸìˆ˜ ë¯¸ì§€ì • ì‹œ ë²¡í„°ìŠ¤í† ì–´ ë‚´ì— í‚¤ì›Œë“œê°€ ë‚¨ì§€ ì•Šì„ ë•Œê¹Œì§€ ë¬´í•œë°˜ë³µí•œë‹¤. ë°˜ë³µíšŸìˆ˜ ì§€ì • ì‹œ í•´ë‹¹ íšŸìˆ˜ë§Œí¼ ë°˜ë³µí•˜ê³  ì¢…ë£Œí•œë‹¤.)
        while True : 
            iteration += 1 # ë°˜ë³µíšŸìˆ˜ 1ì¦ê°€
            random_keywords = self.keyword_ai.vectorstore.similarity_search('')
            if len(random_keywords) == 0 : # ë²¡í„°ìŠ¤í† ì–´ ë‚´ì— í‚¤ì›Œë“œê°€ ë‚¨ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ
                print('ğŸŸ¡ ë²¡í„°ìŠ¤í† ì–´ ë‚´ì— ì €ì¥ëœ ëª¨ë“  í‚¤ì›Œë“œë¥¼ ì†Œì§„í•˜ì˜€ìŠµë‹ˆë‹¤. ê³ ìƒí•˜ì…¨ìŠµë‹ˆë‹¤.')
                break
            try : 
                my_keyword = self.keyword_ai.vectorstore.similarity_search('')[0].page_content # ê¸€ì„ ì“¸ ì²«ë²ˆì§¸ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.    
                my_keywords = self.keyword_ai.vectorstore_extract(
                    my_keyword, 
                    score_threshold=keyword_score_threshold,
                    k = kewyord_retriever_k
                    )     # ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë²¡í„°ìŠ¤í† ì–´ ë‚´ì—ì„œ ì‚­ì œ (extract)
            
            
            
            
            
                self.posting_ai.create_topics(my_keywords, num_topic=num_topic, save=True)     # ì†Œì œëª© 3ê°œ ìƒì„±
                self.posting_ai.create_prologue(self.posting_ai.results['topics'], self.posting_ai.results['keywords'], save=True)     # í”„ë¡¤ë¡œê·¸ ìƒì„±
                self.posting_ai.create_title(self.posting_ai.results['topics'], self.posting_ai.results['keywords'], save=True)     # ì œëª© ìƒì„±
                # ìë£Œìˆ˜ì§‘
                for topic in self.posting_ai.results['topics'] :
                    documents, hrefs = self.crawler.ddgsearch_reducing(topic)    
                    self.posting_ai.results['documents_urls'].append(documents)
                    self.posting_ai.results['documents'].append(hrefs)

                # ê¸ì–´ë“¤ì¸ ê¸€ë“¤ì„ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥í•˜ê¸°
                collected_documents = []
                for document in self.posting_ai.results['documents'] :
                    collected_documents.extend(document)
                self.posting_ai.vectorstore_recursive_save_texts(collected_documents)


                

                # ê¸€ ì‘ì„±
                self.posting_ai.create_content(
                    topics = self.posting_ai.results['topics'], 
                    language=contents_lan, 
                    score_threshold=documents_score_threshold,  
                    k=documents_retriever_k,  
                    save=True)

                # ì´ë¯¸ì§€ ìˆ˜ì§‘í•˜ê¸°    
                topics = self.posting_ai.results['topics']
                for topic in topics : 
                    images = self.crawler.ddgsearch_get_images(topic, max_results = num_images)
                    self.posting_ai.results['images'].append(images)    

                # ê¸€ í¬ë§·íŒ…í•˜ê¸°
                self.posting_ai.create_HTML_formmater(save=True)

                # ê²°ê³¼ì €ì¥í•˜ê¸°

                # ìˆ˜ì§‘ ë° ìƒì„±í•œ ëª¨ë“  ìë£Œë¥¼ jsonìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
                self.file_manager.saveGeneratedDict2Json(
                    data = self.posting_ai.results,
                    subject = self.blogname,
                    language = 'ko',
                    contents = self.posting_ai.results['title'],
                    folder_category='storage'
                )
                # í¬ìŠ¤íŒ…í•  ë¬¸ì„œë¥¼ txtë¡œ ì €ì¥í•©ë‹ˆë‹¤.
                self.file_manager.saveGeneratedText2Text(
                    data = self.posting_ai.results['html_for_upload'],
                    subject = self.blogname,
                    language = 'ko',
                    contents = self.posting_ai.results['title'],
                    folder_category='for_upload'
            )
                # ì €ì¥ í›„ self.posting_aiì˜ results ì´ˆê¸°í™”.
                self.posting_ai.clear()
            except Exception as e :
                print(e)
                continue
            finally :
                if num_contents_creation is not None :
                    if iteration >= num_contents_creation :
                        break


    def upload_contents(self, num_contents_upload = 15) :

        assert len(os.getenv(f'{self.blogname}_ID')) > 0, 'ë¸”ë¡œê·¸ ì•„ì´ë””ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•˜ì„¸ìš”.'
        assert len(os.getenv(f'{self.blogname}_PW')) > 0, 'ë¸”ë¡œê·¸ ë¹„ë°€ë²ˆí˜¸ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•˜ì„¸ìš”.'
        assert len(os.getenv(f'{self.blogname}_NEW_POST_URL')) > 0, 'ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…URLë¥¼ í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•˜ì„¸ìš”.'

        self.uploader.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        # ì—…ë¡œë“œ í•  ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        filenames = self.file_manager.get_file_names() # ì „ì²´ íŒŒì¼ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

        upload_titles = [] # ì—…ë¡œë“œí•  ë¬¸ì„œì˜ ì œëª© ì»¨í…Œì´ë„ˆ
        upload_contents = [] # ì—…ë¡œë“œí•  ë¬¸ì„œì˜ ë‚´ìš© ì»¨í…Œì´ë„ˆ
        
        for filename in filenames[:num_contents_upload] :
            _, _, title, _ = self.file_manager.extract_elements_from_filename(filename) # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ
            text = self.file_manager.loadText2Text_w_filename(filename, 'for_upload') # íŒŒì¼ ë‚´ìš© ë¶ˆëŸ¬ì˜¤ê¸°

            upload_titles.append(title) # ì»¨í…Œì´ë„ˆì— ì œëª© ì¶”ê°€
            upload_contents.append(text)   # ì»¨í…Œì´ë„ˆì— ë‚´ìš© ì¶”ê°€
            self.file_manager.moveComplete(filename) # ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œì´ ì™„ë£Œëœ ì™„ë£Œëœ íŒŒì¼ì€ ì´ë™í•©ë‹ˆë‹¤.

        if self.verbose : print(f"ì—…ë¡œë“œ í•  ë¬¸ì„œì˜ ê°¯ìˆ˜ëŠ” {len(upload_contents)}ê°œ ì…ë‹ˆë‹¤.")
        
        try :
            self.uploader.tistory_upload(
                titles = upload_titles, # ì—…ë¡œë“œí•  ì „ì²´ ì œëª© ëª©ë¡
                contents = upload_contents, # ì—…ë¡œë“œí•  ì „ì²´ ë‚´ìš© ëª©ë¡
                # uploading_day ë¯¸ ì§€ì • ì‹œ ë‚´ì¼ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì—…ë¡œë“œ ,
                uploading_start_hour = 2, # ì—…ë¡œë“œ ì‹œì‘ ì‹œê°„ (ê¸°ë³¸ê°’ 2ì‹œ)
                uploading_minute_term = 120)  # ì—…ë¡œë“œ ê°„ê²© (ê¸°ë³¸ê°’ 120ë¶„)
        finally :
            self.uploader.driver.quit() # ë¦¬ì†ŒìŠ¤ ì ˆì•½ì„ ìœ„í•œ ë“œë¼ì´ë²„ ì¢…ë£Œ
            # ê²½ê³ ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.
import warnings
warnings.filterwarnings("ignore")

# í™˜ê²½ë³€ìˆ˜ ì €ì¥ (.env) í™•ì¸
from dotenv import load_dotenv
import os


load_dotenv() 



# blogger í´ë˜ìŠ¤ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
from module.Blogger import Blogger

# ê¸°ë³¸ì ì¸ í™˜ê²½ì„¤ì •ì„ í•©ë‹ˆë‹¤.
isHeadless = True
verbose = True

# blogname = input('ë¸”ë¡œê·¸ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” ')
print('ğŸ’›')
statifi = Blogger(blogname = 'statifi', verbose = verbose, isHeadless = isHeadless)
print('ğŸ’›')
kindmom = Blogger(blogname = 'kindmom', verbose = verbose, isHeadless = isHeadless)
print('ğŸ’›')
sweetkiwi = Blogger(blogname = 'sweetkiwi', verbose = verbose, isHeadless = isHeadless)
print('ğŸ’›')
takuz = Blogger(blogname = 'takuz', verbose = verbose, isHeadless = isHeadless)
print('ğŸ’›')
morningbbobbo = Blogger(blogname = 'morningbbobbo', verbose = verbose, isHeadless = isHeadless)

bloggers ={
    "statifi" :statifi,
    "kindmom" :kindmom,
    "sweetkiwi" :sweetkiwi,
    "takuz" :takuz,
    "morningbbobbo" :morningbbobbo,
}



recommended = {
    "ê±´ê°•ê³¼ ì›°ë¹™": ["ìš”ê°€", "ë§ˆì¸ë“œí’€ë‹ˆìŠ¤", "ì›°ë‹ˆìŠ¤ ë¦¬íŠ¸ë¦¿", "ê±´ê°•í•œ ê°„ì‹", "ëª…ìƒ", "í•„ë¼í…ŒìŠ¤", "ê±´ê°•í•œ ì‹ë‹¨", "ê±´ê°• ì¶”ì„¸", "íë§", "ê±´ê°• ê²€ì§„"],
    "ê¸°ì—…ê³¼ ê¸°ì—…ê°€": ["ê¸°ì—…ê°€ ì •ì‹ ", "í¬ë¼ìš°ë“œí€ë”©", "ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ í˜ì‹ ", "ê¸°ìˆ  ë²¤ì²˜", "ë²¤ì²˜ íˆ¬ì", "ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„"],
    "ì¬íƒ ê·¼ë¬´ ë° ìê¸° ê³„ë°œ ë¸”ë¡œê·¸": ["ì¬íƒ ê·¼ë¬´ íŒ", "ìê¸°ê³„ë°œ ì±… ì¶”ì²œ", "ì˜¨ë¼ì¸ í•™ìŠµ í”Œë«í¼", "ì‹œê°„ ê´€ë¦¬ ê¸°ìˆ ", "ìƒì‚°ì„± í–¥ìƒ", "ì‚¬ì´ë“œ í—ˆìŠ¬", "ì¬íƒ ê·¼ë¬´ í™˜ê²½", "í”„ë¦¬ëœì„œ ê°€ì´ë“œ", "ìê¸° ê´€ë¦¬", "ëª¨í‹°ë² ì´ì…˜ ì¦ì§„"],
    "ê±´ê°• ë° ì›°ë‹ˆìŠ¤ ë¸”ë¡œê·¸": ["ê±´ê°•í•œ ì‹ë‹¨", "ì •ì‹  ê±´ê°•", "ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬", "í™ˆ í”¼íŠ¸ë‹ˆìŠ¤", "ëª…ìƒ ê¸°ë²•", "ì›°ë‹ˆìŠ¤ ë¼ì´í”„ìŠ¤íƒ€ì¼", "ì˜ì–‘ ì •ë³´", "ê±´ê°• ê²€ì§„ ê°€ì´ë“œ", "ì²´ì¤‘ ê´€ë¦¬", "ê±´ê°•í•œ ìŠµê´€"],
    
}



# ê°€ì • ê´€ë¦¬ ë° DIY í”„ë¡œì íŠ¸ ë¸”ë¡œê·¸
subjects_n_words_for_statifi = {"ê°€ì • ê´€ë¦¬ ë° DIY í”„ë¡œì íŠ¸ ë¸”ë¡œê·¸": ["í™ˆ ë°ì½” ì•„ì´ë””ì–´", "ìë™ì°¨ ê´€ë¦¬", "ì •ì› ê°€ê¾¸ê¸°", "DIY í”„ë¡œì íŠ¸", "ê°€ì • ìœ ì§€ë³´ìˆ˜", "ê°€êµ¬ ì œì‘", "ì§‘ì•ˆ ì •ë¦¬ íŒ", "ì—ë„ˆì§€ íš¨ìœ¨ ê°€ì´ë“œ", "ì•ˆì „í•œ ì§‘ ë§Œë“¤ê¸°", "ìœ ì•„ êµìœ¡ í™œë™"] }
# ìŒì‹ ìš”ë¦¬ 
subjects_n_words_for_kindmom = {"ìŒì‹ê³¼ ìš”ë¦¬": ['ë¯¸ì‰ë¦°', 'ë¯¸ìŠë­', 'ë¸”ë£¨ë¦¬ë³¸', '100ë…„ê°€ê²Œ', "ë² ì§€í…Œë¦¬ì–¸ ë ˆì‹œí”¼", "ìŒì‹ íŠ¸ë Œë“œ", "ìŒì‹ ì‹œì¥", "ë¡œì»¬ ì‹ë‹¹ íƒë°©", "ë””í†¡ìŠ¤ ìš”ë¦¬", "ìˆ˜í¼í‘¸ë“œ", "í–¥í†  ìŒì‹", "ìŒì‹ ë¸”ë¡œê·¸", "ì¿ í‚¹ í´ë˜ìŠ¤", "í‘¸ë“œ íˆ¬ì–´"], }
# ì—¬í–‰
subjects_n_words_for_sweetkiwi = {"ë””ì§€í„¸ ë…¸ë§ˆë“œì™€ ì—¬í–‰ ë¸”ë¡œê·¸": ["ë””ì§€í„¸ ë…¸ë§ˆë“œ íŒ", "ê¸€ë¡œë²Œ ì—¬í–‰ ê°€ì´ë“œ", "ì‘ì—… íš¨ìœ¨ì„±", "ì—¬í–‰ìš© ì•±", "ì—¬í–‰ ì˜ˆì‚° ê´€ë¦¬", "ë¹„ì ê°€ì´ë“œ", "ì—¬í–‰ì ë³´í—˜", "ì‘ì—… ê³µê°„ ì°¾ê¸°", "ë¬¸í™” ì ì‘ íŒ", "ì†Œì…œ ë¯¸ë””ì–´ ë§ˆì¼€íŒ…"],  }
# ì¸ê³µì§€ëŠ¥, 
subjects_n_words_for_takuz = {"ì¸ê³µì§€ëŠ¥": ["ì¸ê³µì§€ëŠ¥ ì‘ìš©", "ë¨¸ì‹  ëŸ¬ë‹", "ë°ì´í„° ë¶„ì„", "ë”¥ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜", "í•€í…Œí¬", "ì—ì½”í…Œí¬", "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ", "í…Œí¬ ë¦¬ë·°"]}
# ì›°ë‹ˆìŠ¤
subjects_n_words_for_morningbbobbo = {"ì§€ì† ê°€ëŠ¥í•œ ìƒí™œ ë¸”ë¡œê·¸": ["ì œë¡œ ì›¨ì´ìŠ¤íŠ¸ íŒ", "ì§€ì† ê°€ëŠ¥í•œ ì œí’ˆ", "ì¹œí™˜ê²½ ìƒí™œ", "ì—ì½” ë¼ì´í”„ìŠ¤íƒ€ì¼", "í™˜ê²½ ë³´í˜¸", "í”Œë¼ìŠ¤í‹± í”„ë¦¬", "ì¬ì‚¬ìš© ê°€ëŠ¥ ì œí’ˆ", "ì¹œí™˜ê²½ í™ˆ ë©”ì´í¬ì˜¤ë²„", "ì§€ì† ê°€ëŠ¥í•œ íŒ¨ì…˜", "ì—ë„ˆì§€ ì ˆì•½ íŒ"], }



depth = 3
save = True


import threading

def collect_keywords_for_blogger(blogger, subjects_n_words, depth, save):
    print(f"Collecting keywords for {blogger.blogname}...")
    blogger.collect_keywords(subjects_n_words=subjects_n_words, depth=depth, save=save)
    print(f"Finished collecting keywords for {blogger.blogname}.")

# ê° ë¸”ë¡œê±°ì˜ ì£¼ì œ ë° í‚¤ì›Œë“œ ë°ì´í„°
bloggers = {
    'statifi': statifi,
    'kindmom': kindmom,
    'sweetkiwi': sweetkiwi,
    'takuz': takuz,
    'morningbbobbo': morningbbobbo
}

subjects_n_words = {
    'statifi': subjects_n_words_for_statifi,
    'kindmom': subjects_n_words_for_kindmom,
    'sweetkiwi': subjects_n_words_for_sweetkiwi,
    'takuz': subjects_n_words_for_takuz,
    'morningbbobbo': subjects_n_words_for_morningbbobbo
}

# ìŠ¤ë ˆë“œ ëª©ë¡ ìƒì„±
threads = []

# ê° ë¸”ë¡œê±°ì— ëŒ€í•´ ìŠ¤ë ˆë“œ ìƒì„± ë° ì‹¤í–‰
for blogger_name, blogger in bloggers.items():
    thread = threading.Thread(target=collect_keywords_for_blogger, args=(blogger, subjects_n_words[blogger_name], depth, save))
    threads.append(thread)
    thread.start()

# ëª¨ë“  ìŠ¤ë ˆë“œì˜ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¼
for thread in threads:
    thread.join()

print("All keyword collection tasks completed.")
