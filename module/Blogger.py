# âœ…todo : 1. colab ìš©ìœ¼ë¡œ requirements.txtì„ í•˜ë‚˜ ë” ë§Œë“ ë‹¤.
# âœ… todo : 1. turnonselenium ë‚´ìš©ì„ colabìš©ìœ¼ë¡œ ì¶”ê°€í•œë‹¤.
# âœ…todo : 1. bloggerë¥¼ ëª¨ë“ˆí™”í•´ì„œ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ ë§Œë“ ë‹¤.
# âœ…todo : 1. ì“°ë ˆë”© ì‹œí‚¤ëŠ” ê²ƒë„ í•¨ìˆ˜í™”í•œë‹¤.
# âœ…todo : 1. ì½”ë©ì€ ì“°ë ˆë”© ëª‡ê°œë¥¼ ì‹œí‚¤ëŠ”ê²Œ ì ë‹¹í•œì§€ë¥¼ í™•ì¸í•œë‹¤ = 2ê°œ (2ì½”ì–´ë¼ì„œ)
# âœ…todo : 1. ì½”ë©í¬ë¡¤ë§ ì‹œì—, user-agent ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤.
# duckduckgo ëŒ€ì²´í’ˆ ì°¾ê¸°



import os
# ì •ì˜í•œ í´ë˜ìŠ¤ë“¤ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
from module.Crawler import Crawler
from module.AIAgent import AIAgent
from module.File_manager import File_manager
from module.Uploader import Uploader
from datetime import datetime, timedelta
import copy


class Blogger() :
    def __init__(self, blogname, verbose = False, isHeadless = True) :         
        self.blogname = blogname # ë¸”ë¡œê·¸ ì´ë¦„
        self.verbose = verbose # ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """ ë°˜ë“œì‹œ íŒŒì¼ë§¤ë‹ˆì €ë¶€í„° í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤. íŒŒì¼ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ê³¼ í•¨ê»˜ ëª¨ë“  í´ë”êµ¬ì¡°ê°€ ìƒì„±ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."""
        # í´ë˜ìŠ¤ í˜¸ì¶œ        
        self.file_manager = File_manager(blogname = blogname, verbose=verbose) # íŒŒì¼ ë§¤ë‹ˆì €
        self.crawler = Crawler(verbose=verbose, isHeadless=isHeadless) # í¬ë¡¤ëŸ¬
        self.posting_ai = AIAgent(index="my_contents", blogname=blogname, parent_path=self.file_manager.parent_path, verbose=verbose) # í¬ìŠ¤íŒ… AI
        self.keyword_ai = AIAgent(index = "my_keywords", blogname=blogname, parent_path=self.file_manager.parent_path, verbose=verbose) # í‚¤ì›Œë“œ AI
        self.uploader = Uploader(
            blogname=blogname, # ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸, í¬ìŠ¤íŒ…URLì„ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.        
            verbose=verbose,
            isHeadless = isHeadless
            )
        
        # ê¸°ì¡´ì— ìˆ˜ì§‘í•˜ì—¬ì„œ ì €ì¥í•´ë‘ì—ˆë˜ í‚¤ì›Œë“œë“¤ì´ ìˆìœ¼ë©´ì€ ë¶ˆëŸ¬ì™€ì„œ crawler.resultsì— ì €ì¥í•©ë‹ˆë‹¤.
        # ê·¸ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê³¼ì •ì—ì„œ ì¤‘ë³µë˜ëŠ” í‚¤ì›Œë“œë¥¼ ì œê±°í•  ìˆ˜ ìˆì„ ë¿ë§Œ ì•„ë‹ˆë¼ ìƒˆë¡­ê²Œ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œê°€ ê¸°ì¡´ í‚¤ì›Œë“œì— ë®ì–´ì”Œì›Œì§€ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.

        collected_keywords_info = self.file_manager.get_keywords(what_keywords='collected_keywords') # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´            
        if collected_keywords_info is not None : # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ê°€ ìˆìœ¼ë©´
            self.crawler.results = copy.deepcopy(collected_keywords_info.T.to_dict()) # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ë¥¼ crawler.resultsì— ì €ì¥í•©ë‹ˆë‹¤.
        else :            
            print('í•´ë‹¹ ë¸”ë¡œê·¸ëŠ” ìˆ˜ì§‘í•œ í‚¤ì›Œë“œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')        
        
        # self.collected_keywords_info = self.file_manager.get_keywords(what_keywords='collected_keywords')
        # self.collected_keywords = self.collected_keywords_info.keys().to_list()
        # self.screened_keywords_info = self.file_manager.get_keywords(what_keywords='collected_keywords')
        # self.screened_keywords = self.screened_keywords_info.keys().to_list()
        # self.suitable_keywords_info = self.file_manager.get_keywords(what_keywords='collected_keywords')
        # self.suitable_keywords = self.suitable_keywords_info.keys().to_list()
    
    # # ì½”ë©ì „ìš© í•¨ìˆ˜
    # def whereami(self) :
    #   path = os.getcwd()
    #   print(path)
    
    # # ì½”ë©ì „ìš© í•¨ìˆ˜
    # def save(self, filenames = None) :
    #   from google.colab import drive
    #   import os
    #   import shutil
    #   drive.mount('/content/drive')
    #   # ì›ë³¸ ë””ë ‰í† ë¦¬ ì„¤ì •
    #   source_dir = '/content'
    #   # ëŒ€ìƒ ë””ë ‰í† ë¦¬ ì„¤ì •
    #   target_dir = '/content/drive/MyDrive/blogging'
    #   if filenames is None :
    #     filenames = os.listdir(source_dir)
    #   try :
    #     filenames.remove('sample_data')
    #     filenames.remove('drive')
    #     filenames.remove('gdrive')
    #   except :
    #     pass
    #   try :
    #     filenames.remove('.config')
    #   except :
    #     pass
    #   try :
    #     filenames.remove('.git')
    #   except :
    #     pass


    #   # '/content/' ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ê³¼ í´ë” ë‚˜ì—´
    #   for item in filenames:
    #       print('now : ', item)
    #       source_path = os.path.join(source_dir, item)
    #       target_path = os.path.join(target_dir, item)

    #       # íŒŒì¼ ë˜ëŠ” í´ë”ë¥¼ Google ë“œë¼ì´ë¸Œë¡œ ë³µì‚¬
    #       if os.path.isfile(source_path):  # íŒŒì¼ì¸ ê²½ìš°
    #           if os.path.exists(target_path):  # ëŒ€ìƒ ê²½ë¡œì— íŒŒì¼ì´ ì´ë¯¸ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°
    #               os.remove(target_path)
    #           shutil.copy(source_path, target_path)
    #       elif os.path.isdir(source_path):  # ë””ë ‰í† ë¦¬ì¸ ê²½ìš°
    #           if os.path.exists(target_path):  # ëŒ€ìƒ ê²½ë¡œì— í´ë”ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ì‚­ì œ í›„ ë³µì‚¬
    #               shutil.rmtree(target_path)
    #           shutil.copytree(source_path, target_path)

    #   print("ëª¨ë“  íŒŒì¼ì´ Google ë“œë¼ì´ë¸Œë¡œ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    # # ì½”ë©ì „ìš© í•¨ìˆ˜
    # def load(self, filenames = None) :
    #   from google.colab import drive
    #   import os
    #   import shutil
    #   drive.mount('/content/drive')
    #   # ëŒ€ìƒ ë””ë ‰í† ë¦¬ ì„¤ì • (ì´ì œ Colabì˜ ë¡œì»¬ ë””ë ‰í† ë¦¬)
    #   target_dir = '/content'
    #   # ì›ë³¸ ë””ë ‰í† ë¦¬ ì„¤ì • (Google ë“œë¼ì´ë¸Œ ë‚´ì˜ ë°±ì—… í´ë”)
    #   source_dir = '/content/drive/MyDrive/blogging'


    #   # Google ë“œë¼ì´ë¸Œì˜ íŒŒì¼ê³¼ í´ë” ë‚˜ì—´
    #   if filenames is None :
    #     filenames = os.listdir(source_dir)
    #   for item in filenames:
    #       print('now : ', item)
    #       source_path = os.path.join(source_dir, item)
    #       target_path = os.path.join(target_dir, item)

    #       # íŒŒì¼ ë˜ëŠ” í´ë”ë¥¼ Colab ë¡œì»¬ë¡œ ë³µì‚¬
    #       if os.path.isfile(source_path):  # íŒŒì¼ì¸ ê²½ìš°
    #           if os.path.exists(target_path):  # ëŒ€ìƒ ê²½ë¡œì— íŒŒì¼ì´ ì´ë¯¸ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°
    #               os.remove(target_path)
    #           shutil.copy(source_path, target_path)
    #       elif os.path.isdir(source_path):  # ë””ë ‰í† ë¦¬ì¸ ê²½ìš°
    #           if os.path.exists(target_path):  # ëŒ€ìƒ ê²½ë¡œì— í´ë”ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ì‚­ì œ í›„ ë³µì‚¬
    #               shutil.rmtree(target_path)
    #           shutil.copytree(source_path, target_path)

    #       print("ëª¨ë“  íŒŒì¼ì´ Google ë“œë¼ì´ë¸Œì—ì„œ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤.")
        


    def collect_keywords(self, subjects_n_words, depth, save=True, collected_keywords_save = True, screened_keywords_save = True, suitable_keywords_save = True, vectorstore_save=True) :
        self.crawler.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        try :
            # í‚¤ì›Œë“œ ìˆ˜ì§‘ì—ëŠ” crawler í´ë˜ìŠ¤ ë‚´ì—ì„œ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆëŠ” ë°˜ë³µë¬¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            # self.crawler.iterate_keyword_crawling_w_multiple_subjects(depth = depth, subjects_n_words=subjects_n_words, save=save) 
            assert type(subjects_n_words) == dict, 'subjects_n_words must be dictionary (key : subject(str), value : list[words])'        
            for subject, words in subjects_n_words.items() :
                assert type(words) == list, 'words must be list'
                assert len(words) > 0, 'words is required'
                assert type(subject) == str, 'subject must be string'
                self.crawler.iterate_keyword_crawling_w_single_subject(depth, words, subject, save)

                if collected_keywords_save : self.file_manager.save_keywords('collected_keywords', self.crawler.results) # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ ì €ì¥
                if screened_keywords_save : self.file_manager.save_keywords('screened_keywords',  self.crawler.results ) # ì„ ë³„í•œ í‚¤ì›Œë“œ ì •ë³´ ì €ì¥

        except Exception as e :
            print(e)
        finally : # ì¤‘ë‹¨í•˜ë”ë¼ë„ í˜„ì¬ì§„í–‰ì‹œì ê¹Œì§€ê°€ ë¡œì»¬ì— ì €ì¥ë©ë‹ˆë‹¤.
            
            # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ ì¤‘ì—ì„œ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ í‚¤ì›Œë“œëŠ” ì œì™¸í•˜ê³  ì €ì¥í•œë‹¤.
            # ê·¸ëŸ°ë° ì´ ê¸°ëŠ¥ì€ ì—¬ê¸°ì— ë“¤ì–´ê°ˆ ê²ƒì´ ì•„ë‹ˆë¼ ê¸°ë³¸ì ì¸ Cralwerì˜ iteration í•¨ìˆ˜ì— ë“¤ì–´ê°€ì•¼ í•œë‹¤.
            self.crawler.results =  self.crawler.load_results().dropna(
                subset = ['num_ads', 'tistory_rank_at_google']).T.to_dict() 
            print('after interuption : good')

            # collected_keywords
            if collected_keywords_save :
                collected_keywords_info = self.crawler.load_results() # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´
                collected_keywords = self.crawler.get_keywords() # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
                self.file_manager.save_keywords('collected_keywords', self.crawler.results) # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ ì €ì¥
                print('after interuption : collected_keywords = good')


            # í‚¤ì›Œë“œ ì„ ë³„
            if screened_keywords_save :
                screened_keywords_info = self.crawler.load_processed_results() # ì„ ë³„í•œ í‚¤ì›Œë“œ ì •ë³´
                screened_keywords = list(screened_keywords_info.index) # ì„ ë³„í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
                self.file_manager.save_keywords('screened_keywords',  self.crawler.results ) # ì„ ë³„í•œ í‚¤ì›Œë“œ ì •ë³´ ì €ì¥
                print('after interuption : screened_keywords = good')

            # í‚¤ì›Œë“œ ì ì •ì„± ê²€ì‚¬
            if suitable_keywords_save :                
                subjects = screened_keywords_info.loc[:,'subject'].to_list() # í‚¤ì›Œë“œ ì ì •ì„± ê²€ì‚¬ë¥¼ ìœ„í•œ ì£¼ì œ ë¦¬ìŠ¤íŠ¸            
                suitable_keywords = []

                # for screened_word, subject in zip(screened_keywords, subjects) :
                #     self.crawler.
                #     suitable_keywords += self.keyword_ai.suitability_checker(subjects, screened_keywords)


                suitable_keywords = self.keyword_ai.suitability_checker(subjects, screened_keywords) # ì ì •ì„± ê²€ì‚¬
                suitable_keywords_info = screened_keywords_info.loc[suitable_keywords] # ì ì •ì„± ê²€ì‚¬ ê²°ê³¼
                # ìˆ˜ì§‘í•œ ì „ì²´ í‚¤ì›Œë“œ ë¡œì»¬í™˜ê²½ì— csvíŒŒì¼ë¡œ ì €ì¥            
                self.file_manager.save_keywords('suitable_keywords', suitable_keywords_info.T.to_dict() ) # ì ì •í•œ í‚¤ì›Œë“œ ì •ë³´ ì €ì¥
                print('after interuption : suitable_keywords = good')
                
            # # í´ë˜ìŠ¤ ë‚´ë¶€ ë³€ìˆ˜ë¡œ ì €ì¥
            # self.collected_keywords_info = collected_keywords_info # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´
            # self.collected_keywords = collected_keywords # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            # self.screened_keywords_info = screened_keywords_info # ì„ ë³„í•œ í‚¤ì›Œë“œ ì •ë³´
            # self.screened_keywords = screened_keywords# ì„ ë³„í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            # self.suitable_keywords_info = suitable_keywords_info # ì ì •í•œ í‚¤ì›Œë“œ ì •ë³´
            # self.suitable_keywords = suitable_keywords # ì ì •í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            # print('after interuption : good4')
            
            
            # í‚¤ì›Œë“œ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ (keyword_ai)            
            if vectorstore_save :
                self.keyword_ai.vectorstore_save_texts(suitable_keywords)

    def create_contents(self, 
                        num_contents_creation = None, 
                        keyword_score_threshold=0.55,
                        kewyord_retriever_k = 16,
                        num_topic = 3,
                        contents_lan = 'English',
                        num_images = 1,
                        documents_score_threshold=0.25,
                        documents_retriever_k = 10) :        

        iteration = 0 # ë°˜ë³µíšŸìˆ˜
        # ë¬´í•œë°˜ë³µ (ë°˜ë³µíšŸìˆ˜ ë¯¸ì§€ì • ì‹œ ë²¡í„°ìŠ¤í† ì–´ ë‚´ì— í‚¤ì›Œë“œê°€ ë‚¨ì§€ ì•Šì„ ë•Œê¹Œì§€ ë¬´í•œë°˜ë³µí•œë‹¤. ë°˜ë³µíšŸìˆ˜ ì§€ì • ì‹œ í•´ë‹¹ íšŸìˆ˜ë§Œí¼ ë°˜ë³µí•˜ê³  ì¢…ë£Œí•œë‹¤.)
        while True : 
            iteration += 1 # ë°˜ë³µíšŸìˆ˜ 1ì¦ê°€
            if self.verbose : print(f"ğŸŒ ê¸€ì„ ì‘ì„±í•  í‚¤ì›Œë“œë¥¼ ê°€ì§€ê³  ì˜¤ëŠ” ì¤‘... ")
            random_keywords = self.keyword_ai.vectorstore.similarity_search('')
            if len(random_keywords) == 0 : # ë²¡í„°ìŠ¤í† ì–´ ë‚´ì— í‚¤ì›Œë“œê°€ ë‚¨ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ
                print('ğŸŸ¡ ë²¡í„°ìŠ¤í† ì–´ ë‚´ì— ì €ì¥ëœ ëª¨ë“  í‚¤ì›Œë“œë¥¼ ì†Œì§„í•˜ì˜€ìŠµë‹ˆë‹¤. ê³ ìƒí•˜ì…¨ìŠµë‹ˆë‹¤.')
                break
            try : 
                my_keyword = self.keyword_ai.vectorstore.similarity_search('')[0].page_content # ê¸€ì„ ì“¸ ì²«ë²ˆì§¸ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.    
                my_keywords = self.keyword_ai.vectorstore_extract(
                    my_keyword, 
                    score_threshold=keyword_score_threshold,
                    k = kewyord_retriever_k
                    )     # ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë²¡í„°ìŠ¤í† ì–´ ë‚´ì—ì„œ ì‚­ì œ (extract)
            
            
            
            
                
                self.posting_ai.create_topics(my_keywords, num_topic=num_topic, save=True)     # ì†Œì œëª© 3ê°œ ìƒì„±
                self.posting_ai.create_prologue(self.posting_ai.results['topics'], self.posting_ai.results['keywords'], save=True)     # í”„ë¡¤ë¡œê·¸ ìƒì„±
                self.posting_ai.create_title(self.posting_ai.results['topics'], self.posting_ai.results['keywords'], save=True)     # ì œëª© ìƒì„±
                # ìë£Œìˆ˜ì§‘
                if self.verbose : print(f"ğŸŒ ê¸€ ê´€ë ¨ ìë£Œ ìˆ˜ì§‘ ì¤‘ ... ")
                for topic in self.posting_ai.results['topics'] :
                    hrefs, documents = self.crawler.ddgsearch_reducing(topic)    
                    # documents, hrefs = self.crawler.ddgsearch_reducing(topic)    
                    self.posting_ai.results['documents_urls'].append(hrefs)
                    self.posting_ai.results['documents'].append(documents)

                # ê¸ì–´ë“¤ì¸ ê¸€ë“¤ì„ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥í•˜ê¸°                
                collected_documents = []
                for document in self.posting_ai.results['documents'] :
                    collected_documents.extend(document)
                self.posting_ai.vectorstore_recursive_save_texts(collected_documents)


                

                # ê¸€ ì‘ì„±
                if self.verbose : print(f"ğŸŒ ê¸€ ì‘ì„± ì¤‘ ... ")
                self.posting_ai.create_content(
                    topics = self.posting_ai.results['topics'], 
                    language=contents_lan, 
                    score_threshold=documents_score_threshold,  
                    k=documents_retriever_k,  
                    save=True)

                # ì´ë¯¸ì§€ ìˆ˜ì§‘í•˜ê¸°    
                if self.verbose : print(f"ğŸŒ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘ ... ")
                topics = self.posting_ai.results['topics']
                for topic in topics : 
                    images = self.crawler.ddgsearch_get_images(topic, max_results = num_images)
                    self.posting_ai.results['images'].append(images)    

                # ê¸€ í¬ë§·íŒ…í•˜ê¸°
                if self.verbose : print(f"ğŸŒ ê¸€ í˜•ì‹í™” ì¤‘ ... ")
                self.posting_ai.create_HTML_formmater(save=True)

                # ê²°ê³¼ì €ì¥í•˜ê¸°
                if self.verbose : print(f"ğŸŒ ê¸€ ì €ì¥ ì¤‘ ... ")
                # ìˆ˜ì§‘ ë° ìƒì„±í•œ ëª¨ë“  ìë£Œë¥¼ jsonìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
                self.file_manager.saveGeneratedDict2Json(
                    data = self.posting_ai.results,
                    subject = self.blogname,
                    language = 'ko',
                    contents = self.posting_ai.results['title'],
                    folder_category='storage'
                )
                # í¬ìŠ¤íŒ…í•  ë¬¸ì„œë¥¼ txtë¡œ ì €ì¥í•©ë‹ˆë‹¤.
                self.file_manager.saveGeneratedText2Text(
                    data = self.posting_ai.results['html_for_upload'],
                    subject = self.blogname,
                    language = 'ko',
                    contents = self.posting_ai.results['title'],
                    folder_category='for_upload'
            )
                # ì €ì¥ í›„ self.posting_aiì˜ results ì´ˆê¸°í™”.
                self.posting_ai.clear()
                self.posting_ai.vectorstore_clear()
            except Exception as e :
                print(e)
                continue
            finally :
                if num_contents_creation is not None :
                    if iteration >= num_contents_creation :
                        break

    def upload_contents(self, num_contents_upload = 15, uploading_day = (datetime.now() + timedelta(days=1)).day, uploading_start_hour = 2, uploading_minute_term =  120) :

        assert len(os.getenv(f'{self.blogname}_ID')) > 0, 'ë¸”ë¡œê·¸ ì•„ì´ë””ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•˜ì„¸ìš”.'
        assert len(os.getenv(f'{self.blogname}_PW')) > 0, 'ë¸”ë¡œê·¸ ë¹„ë°€ë²ˆí˜¸ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•˜ì„¸ìš”.'
        assert len(os.getenv(f'{self.blogname}_NEW_POST_URL')) > 0, 'ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…URLë¥¼ í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•˜ì„¸ìš”.'

        print(f"""
            ğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒ
            uploading_day : {uploading_day}
            uploading_start_hour : {uploading_start_hour}
            uploading_minute_term : {uploading_minute_term}
            ğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒ
            """
        )


        self.uploader.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        # ì—…ë¡œë“œ í•  ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        filenames = self.file_manager.get_file_names() # ì „ì²´ íŒŒì¼ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

        upload_titles = [] # ì—…ë¡œë“œí•  ë¬¸ì„œì˜ ì œëª© ì»¨í…Œì´ë„ˆ
        upload_contents = [] # ì—…ë¡œë“œí•  ë¬¸ì„œì˜ ë‚´ìš© ì»¨í…Œì´ë„ˆ
        
        for filename in filenames[:num_contents_upload] :
            # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ (íŒŒì¼ì œëª©ì— ì˜¤ë¥˜ê°€ ìˆì„ ê²½ìš° ìˆœì„œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.)
            try :              
              _, _, title, _ = self.file_manager.extract_elements_from_filename(filename) 
            except Exception as e :
              continue
            
            text = self.file_manager.loadText2Text_w_filename(filename, 'for_upload') # íŒŒì¼ ë‚´ìš© ë¶ˆëŸ¬ì˜¤ê¸°

            upload_titles.append(title) # ì»¨í…Œì´ë„ˆì— ì œëª© ì¶”ê°€
            upload_contents.append(text)   # ì»¨í…Œì´ë„ˆì— ë‚´ìš© ì¶”ê°€
            self.file_manager.moveComplete(filename) # ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œì´ ì™„ë£Œëœ ì™„ë£Œëœ íŒŒì¼ì€ ì´ë™í•©ë‹ˆë‹¤.

        if self.verbose : print(f"ì—…ë¡œë“œ í•  ë¬¸ì„œì˜ ê°¯ìˆ˜ëŠ” {len(upload_contents)}ê°œ ì…ë‹ˆë‹¤.")
        
        try :
            self.uploader.tistory_upload(
                titles = upload_titles, # ì—…ë¡œë“œí•  ì „ì²´ ì œëª© ëª©ë¡
                contents = upload_contents, # ì—…ë¡œë“œí•  ì „ì²´ ë‚´ìš© ëª©ë¡
                uploading_day  = uploading_day,
                uploading_start_hour = uploading_start_hour, # ì—…ë¡œë“œ ì‹œì‘ ì‹œê°„ (ê¸°ë³¸ê°’ 2ì‹œ)
                uploading_minute_term = uploading_minute_term)  # ì—…ë¡œë“œ ê°„ê²© (ê¸°ë³¸ê°’ 120ë¶„)
        finally :
            self.uploader.driver.quit() # ë¦¬ì†ŒìŠ¤ ì ˆì•½ì„ ìœ„í•œ ë“œë¼ì´ë²„ ì¢…ë£Œ