# í•„ìš”í•œ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸ í•©ë‹ˆë‹¤.
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import copy
import requests
from bs4 import BeautifulSoup
import pandas as pd
from selenium.webdriver.common.action_chains import ActionChains
# ddg
from duckduckgo_search import DDGS
# í¬ë¡¤ëŸ¬ë¼ëŠ” í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class Crawler():
    # ìƒíƒœë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
    def __init__(self, verbose=False, isHeadless=True) :          
        """Crawler í´ë˜ìŠ¤ì˜ ìƒì„±ìë¡œ, ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ ì´ˆê¸° ì„¤ì •ì„ í•©ë‹ˆë‹¤.
        verbose: ë¶€ìš¸ ê°’ìœ¼ë¡œ, True ì„¤ì • ì‹œ ì‹¤í–‰ ê³¼ì •ì—ì„œ ìƒì„¸ ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
        """
        self.driver = self.turn_on_selenium(is_headless=isHeadless)  # ì…€ë ˆë‹ˆì›€ ë¸Œë¼ìš°ì €ë¥¼ í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ì—¬ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        self.verbose = verbose  # ì‹¤í–‰ ê³¼ì •ì—ì„œ ë¡œê·¸ë¥¼ ì¶œë ¥í• ì§€ì˜ ì—¬ë¶€ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        self.base_search_urls = {
            # ê° ê²€ìƒ‰ ì—”ì§„ë³„ ê¸°ë³¸ ê²€ìƒ‰ URLì„ ì„¤ì •í•©ë‹ˆë‹¤. <your_query>ëŠ” ê²€ìƒ‰ì–´ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.
                    "daum" : f'https://search.daum.net/search?nil_suggest=btn&w=tot&DA=SBC&q=<your_query>',
                    "naver" : f'https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query=<your_query>',
                    "google" : f'https://www.google.com/search?q=<your_query>&sca_esv=a19c4e940f5aec6f&rlz=1C1IBEF_koKR958KR958&ei=UjTbZePvLeGP2roPzY2qiAo&udm=&ved=0ahUKEwjj4I7ZwMaEAxXhh1YBHc2GCqEQ4dUDCBA&uact=5&oq=<your_query>&gs_lp=Egxnd3Mtd2l6LXNlcnAiD-yKpO2LsOu4jOyeoeyKpDIFEC4YgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAuGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEC4YgAQyFBAuGIAEGJcFGNwEGN4EGN8E2AEBSLoOUIQFWNQLcAN4ApABA5gBjAGgAdkLqgEEMC4xMrgBA8gBAPgBAZgCCKACvgTCAgQQABhHwgIEEAAYA8ICCxAAGIAEGLEDGIMBwgIEEC4YA8ICERAuGIAEGLEDGIMBGMcBGNEDwgIOEC4YgAQYsQMYxwEY0QPCAgoQLhiABBiKBRhDwgIKEAAYgAQYigUYQ8ICGRAuGIAEGIoFGEMYlwUY3AQY3gQY3wTYAQHCAgsQLhiABBixAxiDAcICChAuGEMYgAQYigXCAhkQLhhDGIAEGIoFGJcFGNwEGN4EGN8E2AEBmAMAiAYBkAYKugYGCAEQARgUkgcDNC40&sclient=gws-wiz-serp',
                    "google_image" : f"https://www.google.com/search?q=<your_query>&tbm=isch&ved=2ahUKEwjNvI38r-uEAxX4fvUHHaL9B4EQ2-cCegQIABAA&oq=<your_query>&gs_lp=EgNpbWciCXN0ZXZlam9iczIFEAAYgAQyBBAAGB4yCRAAGIAEGBgYCjIJEAAYgAQYGBgKMgkQABiABBgYGAoyCRAAGIAEGBgYCkiZ1CBQ8gFYkdMgcAF4AJABAJgBeqABgAmqAQM1Lja4AQPIAQD4AQGKAgtnd3Mtd2l6LWltZ6gCAMICBxAAGIAEGBjCAggQABiABBixA8ICBxAAGIAEGBOIBgE&sclient=img&ei=14juZY31Avj91e8PovufiAg&bih=1023&biw=2048"
                }
        self.tag_selectors = {
            # ê° ê²€ìƒ‰ ì—”ì§„ ë° íƒœê·¸ì— ë”°ë¥¸ CSS ì„ íƒìë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
            # í‚¤ì›Œë“œ
            'daum_open_keywords' :  'a.keyword', # ë‹¤ìŒ ì˜¤í”ˆí‚¤ì›Œë“œ
            'google_open_keywords' : 'div.s75CSd.u60jwe.r2fjmd.AB4Wff', # êµ¬ê¸€ ì˜¤í”ˆí‚¤ì›Œë“œ
            'daum_searchinput' : '#q.tf_keyword', # ë‹¤ìŒ ê²€ìƒ‰ì°½
            'google_searchinput' : '#searchform textarea.gLFyf', # êµ¬ê¸€ ê²€ìƒ‰ì°½
            'daum_suggest_keywords' : "span.txt_query", # ë‹¤ìŒ ì„œì œìŠ¤íŠ¸í‚¤ì›Œë“œ
            'google_suggest_keywords' : 'div.wM6W7d span', # êµ¬ê¸€ ì„œì œìŠ¤íŠ¸í‚¤ì›Œë“œ
            # ë‹¤ìŒê´‘ê³ 
            'daum_AD_normal' : '.list_ad li', #ì™€ì´ë“œê´‘ê³ , í”„ë¦¬ë¯¸ì—„ê´‘ê³ 
            'daum_AD_special' : '#splinkColl .list_info.mg_cont.clear li', # ìŠ¤í˜ì…œê´‘ê³                         
            # êµ¬ê¸€ ì»¨í…ì¸  ìˆœì„œ
            'google_contents_link' : 'div#rso.dURPMd  div.MjjYud a'# êµ¬ê¸€ ê²€ìƒ‰ê²°ê³¼ ë§í¬ 
            
        }
        self.basic_format = {
            # ë°ì´í„° ìˆ˜ì§‘ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ë°ì´í„° êµ¬ì¡°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
            'source' : [],
            'subject' : None,
            'num_ads' : None,            
            "tistory_rank_at_google" : None,
            'all_links' : None,
            "top_tistory_at_google" : None,
            'images':None
        }
        self.results = {} # ìˆ˜ì§‘ëœ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.
    

    
    
    def load_results(self) :
        """ìˆ˜ì§‘ëœ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return pd.DataFrame(self.results).T 
    def get_keywords(self) :
        """ì‚¬ìš©ìì—ê²Œ ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘í•œ í‚¤ì›Œë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return list(self.results.keys())
    
    def load_processed_results(self, num_ads = 7, tistory_rank_at_google = 10) :
        """ìˆ˜ì§‘ëœ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
          ìƒì—…ì„± í‚¤ì›Œë“œì™€ google ì— tistory ë…¸ì¶œì´ 10ë²ˆì§¸ ì´ìƒì¸ í‚¤ì›Œë“œëŠ” ì œê±°í•©ë‹ˆë‹¤."""
        df = pd.DataFrame(copy.deepcopy(self.results)).T # ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ì„ ê¹Šì€ ë³µì‚¬ë¥¼ í†µí•´ ìƒì„±í•©ë‹ˆë‹¤.
        df = df[df['num_ads'] < num_ads] # ìƒì—…ì„± í‚¤ì›Œë“œ ì œê±°

        def safe_int_convert(x):
            try:
                return int(x)
            except ValueError:
                return None  # ìˆ«ìë¡œ ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ê²½ìš° None ë°˜í™˜
        
        df["tistory_rank_at_google"] = df["tistory_rank_at_google"].apply(safe_int_convert)
        df = df[df["tistory_rank_at_google"] < tistory_rank_at_google] # google ì— tistory ë…¸ì¶œ 10ë²ˆì§¸ ì´ìƒì œê±°
        df.sort_values(by='tistory_rank_at_google', ascending=True,inplace=True) # ìˆœìœ„ë³„ ì •ë ¬
        return df
    
    def get_processed_keywords(self, num_ads = 7, tistory_rank_at_gogle = 10) :
        """ìˆ˜ì§‘ëœ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
          ìƒì—…ì„± í‚¤ì›Œë“œì™€ google ì— tistory ë…¸ì¶œì´ 10ë²ˆì§¸ ì´ìƒì¸ í‚¤ì›Œë“œëŠ” ì œê±°í•©ë‹ˆë‹¤."""
        df = self.load_processed_results(num_ads = num_ads, tistory_rank_at_gogle = tistory_rank_at_gogle)
        return list(df.index)
     
    
    
    def clear(self) :
        """ìˆ˜ì§‘ëœ ê²°ê³¼ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.results = {}

    def createURL(self, word, engine) :        
        """ê³µë°±ì€ +ë¡œ ë°”ê¾¸ì–´ì„œ ê° ê²€ìƒ‰ì—”ì§„ì˜ ê²€ìƒ‰ì°½ì— í•´ë‹¹í•˜ëŠ” URLì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        word = word.replace(" ", "+") # ê³µë°±ì„ +ë¡œ ë°”ê¾¸ì–´ì¤ë‹ˆë‹¤.
        base_url = self.base_search_urls.get(engine) # í•´ë‹¹ ê²€ìƒ‰ ì—”ì§„ì˜ ê¸°ë³¸ URLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        if base_url:
            return base_url.replace("<your_query>", word) # <your_query>ë¥¼ wordë¡œ ë°”ê¾¸ì–´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
        else: # ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—”ì§„ì´ë©´ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  ë§í•´ì¤ë‹ˆë‹¤.
            print("ì§€ì›ë˜ì§€ ì•ŠëŠ” ì—”ì§„ì…ë‹ˆë‹¤.")
            return
    
    # ì…€ë ˆë‹ˆì›€ ì¼œê¸°
    def turn_on_selenium(self, is_headless=True):
        """ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ë¥¼ ì´ˆê¸°í™”í•˜ê³  í¬ë¡¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        is_headless: ë¶€ìš¸ ê°’ìœ¼ë¡œ, Trueì¼ ê²½ìš° í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ í¬ë¡¬ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ ìë™ í˜¸ì¶œë©ë‹ˆë‹¤."""
        chrome_options = Options() # í¬ë¡¬ ì˜µì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.         
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        # ì‚¬ìš©ì ì—ì´ì „íŠ¸ ì„¤ì •
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
        chrome_options.add_argument(f"user-agent={user_agent}")
        if is_headless: # headless : í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì„¤ì •í•©ë‹ˆë‹¤. (ëˆˆì— ë³´ì´ëŠ” í¬ë¡¬í™”ë©´ì„ í‘œì‹œí•˜ì§€ ì•ŠìŒìœ¼ë¡œì¨ ë¦¬ì†ŒìŠ¤ë¥¼ ì•„ë‚ë‹ˆë‹¤.)
            chrome_options.add_argument("--headless")         
        driver = webdriver.Chrome(options=chrome_options)  # í¬ë¡¬ì„ ì‹¤í–‰ì‹œí‚µë‹ˆë‹¤.               
        return driver # í¬ë¡¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    # ì…€ë ˆë‹ˆì›€ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸
    def is_selenium_turned_on(self):
        """ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆì§€ ì•Šìœ¼ë©´ ì¼œê³ , ì¼œì ¸ìˆìœ¼ë©´ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."""
        if self.driver is None : 
            self.driver = self.turn_on_selenium() # ìœ„ì—ì„œ ì •ì˜í•œ ì…€ë ˆë‹ˆì›€ ì‹¤í–‰í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        return

    def search(self, word, engine) :
        """ì…€ë ˆë‹ˆì›€ì„ ì¡°ì‘í•˜ì—¬ í•´ë‹¹ ì—”ì§„ì˜ ê²€ìƒ‰ì°½ì— wordë¥¼ ê²€ìƒ‰í•œ ìƒíƒœì˜ í˜ì´ì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        self.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸   
        self.driver.get(self.createURL(word, engine)) # í•´ë‹¹ ì—”ì§„ì˜ ê²€ìƒ‰ì°½ì— wordë¥¼ ê²€ìƒ‰í•œ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.

    # ì…€ë ˆë‹ˆì›€ í´ë¦­ì´ë²¤íŠ¸ ì¼ìœ¼í‚¤ê¸°
    def selenium_click_action(self, tag_css_selector):
        """ì…€ë ˆë‹ˆì›€ì„ ì‚¬ìš©í•˜ì—¬ CSS ì¿¼ë¦¬ ì„ íƒìì— ë”°ë¥¸ í´ë¦­ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        self.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸        
        # if self.verbose : print(f"ğŸ’¬ í´ë¦­")
        # ì´ì œ driverë¥¼ ì‚¬ìš©í•˜ì—¬ CSS ì¿¼ë¦¬ ì„ íƒìì— ë”°ë¥¸ í´ë¦­ ì‘ì—… ìˆ˜í–‰
        # í´ë¦­í•˜ë ¤ëŠ” íƒœê·¸ê°€ í™”ë©´ì— í‘œì‹œë˜ê¸°ë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
        WebDriverWait(self.driver, 3).until(EC.presence_of_element_located((By.CSS_SELECTOR, tag_css_selector)))
        element = self.driver.find_element(By.CSS_SELECTOR, tag_css_selector) # í´ë¦­í•˜ë ¤ëŠ” íƒœê·¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
        element.click() # í´ë¦­í•©ë‹ˆë‹¤.
        # if self.verbose : print(f"âœ… í´ë¦­")
    
    def selenium_crawling(self, tag_css_selector, get_attribute = None) :
        """ì…€ë ˆë‹ˆì›€ì„ ì‚¬ìš©í•˜ì—¬ CSS ì¿¼ë¦¬ ì„ íƒìì— ë”°ë¥¸ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        self.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸        
        # if self.verbose : print(f"ğŸ’¬ ìŠ¤í¬ë˜í•‘")
        elements = self.driver.find_elements(By.CSS_SELECTOR, tag_css_selector)   # í´ë¦­í•˜ë ¤ëŠ” íƒœê·¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
        if get_attribute != None : # í…ìŠ¤íŠ¸ ì™¸ ë‹¤ë¥¸ ì†ì„±ê°’ì„ ìˆ˜ì§‘í•˜ê³  ì‹¶ë‹¤ë©´
            result = [element.get_attribute(get_attribute) for element in elements if element.text != '']        
        else : # í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ê³  ì‹¶ì€ ê²½ìš°
            result = [element.text for element in elements if element.text != '']        
        # if self.verbose : print('âœ… ìŠ¤í¬ë˜í•‘', len(result))
        return result
    
    def add_new_keyword(self, keywords, source, subject = None) :
        """ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ë˜ ìƒˆë¡œìš´ í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘í•œ ê²½ìš° self.resultsì— í•´ë‹¹ ë‹¨ì–´ì˜ ìˆ˜ì§‘ê³µê°„ì„ ìƒì„±í•©ë‹ˆë‹¤.
          ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í‚¤ì›Œë“œë¼ë©´ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."""
        if type(keywords) == str :
            keywords = [keywords] # í‚¤ì›Œë“œê°€ ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        for keyword in keywords :
            if keyword not in self.results.keys() :
                self.results[keyword] = copy.deepcopy(self.basic_format) # ê¸°ë³¸ ë°ì´í„° í˜•ì‹ì„ ë³µì‚¬í•˜ì—¬ ìƒˆ í‚¤ì›Œë“œì— í• ë‹¹í•©ë‹ˆë‹¤.
            self.results[keyword]['source'].append(source) # ë°ì´í„° ì¶œì²˜ ì¶”ê°€
            if subject is not None : self.results[keyword]['subject'] = subject # ë°ì´í„° ì¶œì²˜ ì¶”ê°€
    
    def save_results(self, word, key, data) :
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ self.resultsì— ì €ì¥í•©ë‹ˆë‹¤. 
        ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í‚¤ì›Œë“œë¼ë©´ í•´ë‹¹ í‚¤ì›Œë“œì˜ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.                
        """
        if word not in self.results.keys() :
            self.results[word] = copy.deepcopy(self.basic_format)
        self.results[word][key] = data

    def crawl_suggest_keywords(self, word, engine, subject=None, save=False) :
        """ì£¼ì–´ì§„ ë‹¨ì–´ì— ëŒ€í•´ ì£¼ì–´ì§„ ì—”ì§„ì—ì„œ ì„œì œìŠ¤íŠ¸ í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        if self.verbose : print(f'{word}ğŸ’¬ {engine}ì—ì„œ suggest keyword í™•ì¸')                        
        self.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸        
        self.search(word, engine)  # ì£¼ì–´ì§„ ê²€ìƒ‰ì–´ë¡œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        self.selenium_click_action(self.tag_selectors.get(f'{engine}_searchinput')) # í´ë¦­
        suggest_keywords =  self.selenium_crawling(self.tag_selectors.get(f'{engine}_suggest_keywords'))   # ë‚´ìš© ìŠ¤í¬ë˜í•‘
        if save : self.add_new_keyword(suggest_keywords, f"{engine}_suggest", subject)  # ìˆ˜ì§‘ëœ ì œì•ˆ í‚¤ì›Œë“œë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        if self.verbose : print(f'{word}âœ… {engine}ì—ì„œ suggest keyword í™•ì¸')  # ë””ë²„ê¹…í•©ë‹ˆë‹¤.
        return  suggest_keywords   
    
    def crawl_open_keywords(self, word, engine, subject = None, save=False) :
        """ì£¼ì–´ì§„ ë‹¨ì–´ì— ëŒ€í•´ ì§€ì •ëœ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ì˜¤í”ˆ í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
            word: ê²€ìƒ‰ì–´
            engine: ì‚¬ìš©í•  ê²€ìƒ‰ ì—”ì§„
            save: ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì €ì¥í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” í”Œë˜ê·¸
            """
        if self.verbose : print(f'{word}ğŸ’¬ {engine}ì—ì„œ open keyword í™•ì¸')                        
        self.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸        
        self.search(word, engine) # ì£¼ì†Œì´ë™
        open_keywords =  self.selenium_crawling(self.tag_selectors.get(f'{engine}_open_keywords'))   # ë‚´ìš© ìŠ¤í¬ë˜í•‘
        if save : self.add_new_keyword(open_keywords, f"{engine}_open", subject) # save=True ì„¤ì •í•œ ê²½ìš° ìˆ˜ì§‘ëœ í‚¤ì›Œë“œë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        if self.verbose : print(f'{word}âœ… {engine}ì—ì„œ open keyword í™•ì¸') 
        return open_keywords
    
    def count_daum_ads(self, words, save=False) :
        """ì£¼ì–´ì§„ ë‹¨ì–´ ë˜ëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ë‹¤ìŒ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ê´‘ê³  ìˆ˜ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        words: ë‹¨ì–´ ë˜ëŠ” ë‹¨ì–´ì˜ ë¦¬ìŠ¤íŠ¸
        save: ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì €ì¥í• ì§€ ì—¬ë¶€
        """
        self.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸ 
        if type(words) == str : # ë‹¨ì¼ ë‹¨ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë¦¬ìŠ¤íŠ¸ë„ ì¸ìë¡œ ë°›ì„ ìˆ˜ ìˆê¸° ìœ„í•´ì„œ!)
            words = [words]
        for word in words :
            time.sleep(1) # ì„œë²„ì— ë¶€í•˜ë¥¼ ì£¼ì§€ ì•Šê¸° ìœ„í•´ ê° ìš”ì²­ ì‚¬ì´ì— íœ´ì‹ ì‹œê°„ì„ ë‘¡ë‹ˆë‹¤.
            self.search(word, 'daum') # ë‹¤ìŒ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            if self.verbose : print(f'{word}ğŸ’¬  ë‹¤ìŒ ê´‘ê³ í™•ì¸')
            num_advertisement = 0 # ê´‘ê³  ìˆ˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
            tags = [self.tag_selectors.get('daum_AD_normal'), self.tag_selectors.get('daum_AD_special')] # ê´‘ê³ ì„¹ì…˜ íƒœê·¸ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
            for tag in tags : 
                try :        
                    ads = self.selenium_crawling(tag)  # ê´‘ê³  íƒœê·¸ì— í•´ë‹¹í•˜ëŠ” ìš”ì†Œë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
                    number_of_ads = len(ads)  # í¬ë¡¤ë§í•œ ê´‘ê³ ì˜ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
                except Exception as e:
                    number_of_ads = 0
                num_advertisement += number_of_ads  # ì´ ê´‘ê³  ìˆ˜ì— ë”í•©ë‹ˆë‹¤.
            if self.verbose : print(F'{word}âœ… ë‹¤ìŒ ê´‘ê³ í™•ì¸ ({num_advertisement})')
            if save : self.save_results(word, 'num_ads', num_advertisement) # ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        
    
    def get_top_tistory_rank_n_link_at_google(self, words, save=False) :
        """êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì£¼ì–´ì§„ ë‹¨ì–´ì— ëŒ€í•´ í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ì˜ ìˆœìœ„ì™€ ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        words: ë‹¨ì–´ ë˜ëŠ” ë‹¨ì–´ì˜ ë¦¬ìŠ¤íŠ¸
        save: ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì €ì¥í• ì§€ ì—¬ë¶€
        """
        self.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸                      
        if type(words) == str :
            words = [words] # ë‹¨ì¼ ë‹¨ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë¦¬ìŠ¤íŠ¸ë„ ì¸ìë¡œ ë°›ì„ ìˆ˜ ìˆê¸° ìœ„í•´ì„œ!)

        for word in words :
            if self.verbose : print(f'{word}ğŸ’¬ êµ¬ê¸€ì—ì„œ í‹°ìŠ¤í† ë¦¬ ìˆœìœ„ í™•ì¸')
            time.sleep(1)# ë„ˆë¬´ ë¹ ë¥¸ í¬ë¡¤ë§ì€ í¬í„¸ ì‚¬ì´íŠ¸ë¡œë¶€í„° ì˜ì‹¬ì„ ì‚¬ì„œ ì œì¬ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§¤ í¬ë¡¤ë§ ì‚¬ì´ì— 1ì´ˆ ì‰¬ì–´ì¤ë‹ˆë‹¤.
            self.search(word, 'google')
            # êµ¬ê¸€ ì²«ë²ˆì§¸ í™”ë©´ì— í‘œì‹œë˜ëŠ” ë§í¬ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
            links = self.selenium_crawling(self.tag_selectors.get('google_contents_link'), get_attribute = 'href')
            rank, href, found = 1, '', False  # í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ì˜ ìˆœìœ„, ë§í¬, ë°œê²¬ ì—¬ë¶€ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ìˆœë²ˆì€ 1ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.
            try :            
                for link in links :                    
                    if 'tistory.com' in link : # ìˆ˜ì§‘í•œ ë§í¬ì— tistoryë¼ëŠ” ì´ë¦„ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
                        href = link
                        found = True # í‹°ìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ í˜„ì¬  ìˆœë²ˆì„ ë°˜í™˜í•©ë‹ˆë‹¤.
                        break# í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ë§í¬ê°€ ë°œê²¬ë˜ë©´ ìˆœìœ„ë¥¼ ê¸°ë¡í•˜ê³  ë°˜ë³µì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
                    else :
                        rank += 1 # í‹°ìŠ¤í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìˆœë²ˆì„ +1í•©ë‹ˆë‹¤.
                if not found :
                    rank = 'ë…¸ì¶œì—†ìŒ'# ì²«ë²ˆì§¸ í™”ë©´ì— í‹°ìŠ¤í† ë¦¬ê°€ ì—†ìœ¼ë©´ 'ë…¸ì¶œì—†ìŒ'ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
                    href = 'ë…¸ì¶œì—†ìŒ'
                if self.verbose : print(f'{word}âœ… êµ¬ê¸€ì—ì„œ í‹°ìŠ¤í† ë¦¬ ìˆœìœ„ í™•ì¸ ({rank})')
            except Exception as e :
                if self.verbose : print(f'{word}ğŸš« êµ¬ê¸€ì—ì„œ í‹°ìŠ¤í† ë¦¬ ìˆœìœ„ í™•ì¸ :',e)
                rank = 'ë…¸ì¶œì—†ìŒ'# ì²«ë²ˆì§¸ í™”ë©´ì— í‹°ìŠ¤í† ë¦¬ê°€ ì—†ìœ¼ë©´ 'ë…¸ì¶œì—†ìŒ'ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
                href = 'ë…¸ì¶œì—†ìŒ'
            finally :                
                if save : # ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
                    self.save_results(word, 'tistory_rank_at_google', rank)  # í‹°ìŠ¤í† ë¦¬ ìˆœìœ„ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
                    self.save_results(word, 'top_tistory_at_google', href)  # í‹°ìŠ¤í† ë¦¬ ë§í¬ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
                    self.save_results(word, 'all_links', links)  # ëª¨ë“  ë§í¬ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

        
    
    
    
    

    def iterate_keyword_crawling_w_single_subject(self, depth, words, subject=None, save=True):
        if not save: 
            temp = copy.deepcopy(self.load_results())
            self.results = {}

        assert depth > 0, 'depth must be greater than 0'
        assert len(words) > 0, 'words is required'

        if type(words) == str:
            words = [words]  # ë‹¨ì¼ ë‹¨ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        # í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        processed_keywords = set(copy.deepcopy(self.get_keywords()))
        new_keywords = set(words)  # ì‹œì‘í•  ìƒˆ í‚¤ì›Œë“œë“¤ì„ ì„¤ì •í•©ë‹ˆë‹¤.

        for current_depth in range(depth):
            print(f'ğŸ’› Now  : Subject : ({subject}) Depth ({current_depth}) collected_keywords : ({len(self.get_keywords())})')  # ë””ë²„ê¹…ìš© ì¶œë ¥ (í˜„ì¬ depth)
                        

            # ë‹¤ìŒ ê¹Šì´ì—ì„œ ì²˜ë¦¬í•  ìƒˆ í‚¤ì›Œë“œë¥¼ ì €ì¥í•  ì„ì‹œ ì§‘í•©
            next_new_keywords = set()

            while new_keywords:
                new_keyword = new_keywords.pop()
                for engine in ['daum', 'google']:
                    try:
                        self.crawl_suggest_keywords(new_keyword, engine, subject, save)
                        self.crawl_open_keywords(new_keyword, engine, subject, save)
                    except Exception as e:
                        print(f'Error occurred while crawling {new_keyword} on {engine}: {e}')
                        continue  # ì‹¤íŒ¨í•œ í‚¤ì›Œë“œëŠ” ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë¡œê¹…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

                # ë‹¤ìŒ ê¹Šì´ì— ì‚¬ìš©í•  ìƒˆ í‚¤ì›Œë“œë“¤ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
                current_keywords = set(self.get_keywords())  # í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ëª¨ë“  í‚¤ì›Œë“œ
                new_discovered_keywords = current_keywords - processed_keywords
                next_new_keywords.update(new_discovered_keywords)
                

                # ê´‘ê³  ìˆ˜ì™€ í‹°ìŠ¤í† ë¦¬ ìˆœìœ„ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
                try:
                    self.count_daum_ads(new_discovered_keywords, save)
                    self.get_top_tistory_rank_n_link_at_google(new_discovered_keywords, save)
                except Exception as e:
                    print(f'Error occurred while collecting info for {new_keyword}: {e}')
                
                processed_keywords.update(new_discovered_keywords)
                

            # ë‹¤ìŒ ê¹Šì´ë¥¼ ìœ„í•´ ìƒˆ í‚¤ì›Œë“œ ì„¸íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
            new_keywords = next_new_keywords
            

        if not save:
            this_result = copy.deepcopy(self.load_results())
            self.results = temp
            return this_result
        

    def iterate_keyword_crawling_w_multiple_subjects(self, depth, subjects_n_words, subject = None, save=True) :
        assert type(subjects_n_words) == dict, 'subjects_n_words must be dictionary (key : subject(str), value : list[words])'
        
        for subject, words in subjects_n_words.items() :
            assert type(words) == list, 'words must be list'
            assert len(words) > 0, 'words is required'
            assert type(subject) == str, 'subject must be string'
            self.iterate_keyword_crawling_w_single_subject(depth, words, subject, save)



    def clean_blog_content(self, text):
        """ë¸”ë¡œê·¸ ì»¨í…ì¸ ë¥¼ ì •ì œí•©ë‹ˆë‹¤."""
        cleaned_text = '\n'.join(filter(None, text.split('\n'))) # ë¹ˆ ì¤„ì„ ì œê±°í•©ë‹ˆë‹¤.
        cleaned_text = '\t'.join(filter(None, cleaned_text.split('\t'))) # ë¹ˆ íƒ­ì„ ì œê±°í•©ë‹ˆë‹¤.
        return cleaned_text
    
    def get_naver_blog_contents_from_urls(self, urls, driver):
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ ë³¸ë¬¸ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ ê²½ìš° ë³¸ë¬¸ ì•ˆì— ë³„ë„ì˜ í”„ë ˆì„ì´ ìˆì–´ì„œ ë³„ë„ì˜ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
        """
        body_contents = [] # ë„¤ì´ë²„ ë³¸ë¬¸ì„ ë‹´ì„ ì»¨í…Œì´ë„ˆì…ë‹ˆë‹¤.
        for link in urls:
            try:
                driver.get(link) # í•´ë‹¹ ë§í¬ë¡œ ì´ë™í•©ë‹ˆë‹¤.
                WebDriverWait(driver, 10).until(
                    EC.frame_to_be_available_and_switch_to_it((By.ID, 'mainFrame'))) # ë¸”ë¡œê·¸ì˜ ë³¸ë¬¸ì´ ìˆëŠ” í”„ë ˆì„ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.
                body_text = driver.find_element(By.TAG_NAME, 'body').text
                driver.switch_to.default_content() # html ë‚´ë¶€ì˜ frameì„ ë²—ì–´ë‚˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
                body_contents.append(body_text) # ë³¸ë¬¸ì„ ì»¨í…Œì´ë„ˆì— ë‹´ìŠµë‹ˆë‹¤.
            except Exception as e:
                continue
        return body_contents
    
    def get_text_from_webpage(self, url, driver=None):
        """ì›¹í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        try:
            response = requests.get(url)
            response.encoding = 'utf-8'

            if response.status_code == 200: # ì •ìƒì ìœ¼ë¡œ ì‘ë‹µì´ ì™”ì„ ë•Œ
                soup = BeautifulSoup(response.text, 'html.parser') # htmlì„ íŒŒì‹±í•©ë‹ˆë‹¤.
                text = soup.get_text() # í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
                return text
            elif 'blog.naver' in url: # ë„¤ì´ë²„ ë¸”ë¡œê·¸ì¸ ê²½ìš°
                return self.get_naver_blog_contents_from_urls([url], driver)[0] # ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ ë³¸ë¬¸ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        except Exception as e:
            if 'blog.naver' in url:
                return self.get_naver_blog_contents_from_urls([url], driver)[0]
        return None

    

    def ddgsearch_reducing(self, keyword, how_many_retrieve=5):
        """ì›¹ë¸Œë¼ìš°ì € ë•ë•ê³ ë¡œë¶€í„° í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        ê·¸ ë‹¤ìŒ í•´ë‹¹ ë§í¬ë“¤ë¡œë¶€í„° ë³¸ë¬¸ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ìˆ˜ì§‘í•œ ìë£ŒëŠ” ì´í›„ì— RAGì— í™œìš©í•©ë‹ˆë‹¤."""
        urls, contents = [], [] # ì‚¬ì´íŠ¸ ì£¼ì†Œì™€ ë‚´ìš©ì„ ë‹´ì„ ì»¨í…Œì´ë„ˆë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.
        try:
            results = DDGS().text(keyword, max_results=20) # ë•ë•ê³ ì—ì„œ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
            urls = [a['href'] for a in results if 'html' not in a['href'] and a['href'] != '']
        except Exception as e:
            print(f'{e} with {keyword}')

        
        try:
            idx, clear = 0, 0 # í…ìŠ¤íŠ¸ ìˆ˜ì§‘ì— ì„±ê³µí•œ ê°¯ìˆ˜ë¥¼ ë‹´ì„ ì»¨í…Œì´ë„ˆ ì…ë‹ˆë‹¤.
            while clear < how_many_retrieve and idx < len(urls):
                url = urls[idx]
                content = self.get_text_from_webpage(url, self.driver) # í•´ë‹¹ ë§í¬ë¡œë¶€í„° ë³¸ë¬¸ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
                if content:
                    contents.append(self.clean_blog_content(content)) # ìˆ˜ì§‘í•œ ë³¸ë¬¸ì„ ì •ì œí•©ë‹ˆë‹¤.
                    clear += 1
                idx += 1
        except Exception as e:
            print(e)
        return urls[:how_many_retrieve], contents

    def ddgsearch_get_images(self, word, query_lan = 'en', region = 'wt-wt', license_image='public', max_results = 1,save = False) : 
        """ì›¹ë¸Œë¼ìš°ì € ë•ë•ê³ ë¡œë¶€í„° í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        if self.verbose : print(f'{word}ğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘')
        word = DDGS().translate(keywords= word, to=query_lan)[0]['translated'] # í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
        images = DDGS().images(keywords = word, region = region, license_image = license_image) # ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        image_container = []
            
        for image in images :
            if 'https' in image['image'] :
                image_container.append(image)
            if len(image_container) == max_results :
                break
        
        if self.verbose : 
            if len(image_container) == max_results :
                print(f'{word}âœ… ì´ë¯¸ì§€ìˆ˜ì§‘ ({len(image_container)})')
            else :
                print(f'{word}âŒ ì´ë¯¸ì§€ìˆ˜ì§‘ ì‹¤íŒ¨({len(image_container)})')
        
        if save : self.save_results(word, 'images', image_container)

        return image_container
    

