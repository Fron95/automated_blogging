{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ…todo : 1. colab ìš©ìœ¼ë¡œ requirements.txtì„ í•˜ë‚˜ ë” ë§Œë“ ë‹¤.\n",
    "# âœ… todo : 1. turnonselenium ë‚´ìš©ì„ colabìš©ìœ¼ë¡œ ì¶”ê°€í•œë‹¤.\n",
    "# âœ…todo : 1. bloggerë¥¼ ëª¨ë“ˆí™”í•´ì„œ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ ë§Œë“ ë‹¤.\n",
    "# âœ…todo : 1. ì“°ë ˆë”© ì‹œí‚¤ëŠ” ê²ƒë„ í•¨ìˆ˜í™”í•œë‹¤.\n",
    "# âœ…todo : 1. ì½”ë©ì€ ì“°ë ˆë”© ëª‡ê°œë¥¼ ì‹œí‚¤ëŠ”ê²Œ ì ë‹¹í•œì§€ë¥¼ í™•ì¸í•œë‹¤ = 2ê°œ (2ì½”ì–´ë¼ì„œ)\n",
    "# âœ…todo : 1. ì½”ë©í¬ë¡¤ë§ ì‹œì—, user-agent ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤.\n",
    "# todo : duckduckgo ê´€ë ¨ìë£Œ ì„œì¹­ ëŒ€ì²´í’ˆ ì°¾ê¸°\n",
    "# todo : ì‰¬ëŠ” ì‹œê°„ ê°€ì§€ë„ë¡ ë§Œë“¤ê¸°\n",
    "# todo : ë§Œë“¤ì–´ì§„ ì»¨í…ì¸ ë“¤ ì¤‘ì— document ì—†ëŠ” ê²½ìš° ì°¾ê¸°\n",
    "\n",
    "\n",
    "# âœ…todo : 1ë”¸ê¹ í…ŒìŠ¤íŠ¸\n",
    "# todo : 1ë”¸ê¹ ë°˜ë³µë¬¸ì— ì‰¬ëŠ” ì‹œê°„ ë¶€ì—¬í•˜ê¸°\n",
    "# todo : 2ë”¸ê¹ í…ŒìŠ¤íŠ¸\n",
    "# todo : 2ë”¸ê¹ ë°˜ë³µë¬¸ì— ì‰¬ëŠ” ì‹œê°„ ë¶€ì—¬í•˜ê¸°\n",
    "# todo : 3ë”¸ê¹ í…ŒìŠ¤íŠ¸\n",
    "# todo : 3ë”¸ê¹ ë°˜ë³µë¬¸ì— ì‰¬ëŠ” ì‹œê°„ ë¶€ì—¬í•˜ê¸°\n",
    "\n",
    "# âœ…todo : 1ë”¸ê¹ í…ŒìŠ¤íŠ¸ - colab\n",
    "# todo : 2ë”¸ê¹ í…ŒìŠ¤íŠ¸ - colab\n",
    "# todo : 3ë”¸ê¹ í…ŒìŠ¤íŠ¸ - colab\n",
    "\n",
    "# todo : ì¶”ê°€ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‘ì—… ì‹œ ê¸°ì¡´ csvë¥¼ ë®ì–´ì“¸ì§€ ë§ì§€ë¥¼ ê²°ì •í•˜ë„ë¡ í•´ì£¼ì„¸ìš”. - í˜„ì¬ëŠ” ë®ì–´ì”Œìš°ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ë˜ì–´ìˆìŒ.\n",
    "# todo : ì¶”ê°€ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‘ì—… ì‹œ ê¸°ì¡´ suitable keywords ë¦¬ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•˜ë„ë¡ í•´ì£¼ì„¸ìš”. (ìˆìœ¼ë©´ ì¬ë°˜ë³µí•´ì„œ ìˆ˜í–‰í•˜ì§€ëŠ” ì•ŠëŠ” ê²ƒìœ¼ë¡œ.)\n",
    "\n",
    "# todo : ì—…ë¡œë“œê°€ ì„±ê³µì ìœ¼ë¡œ ìˆ˜í–‰ë˜ë©´ì€ ì˜®ê¸°ëŠ” ê²ƒìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”.\n",
    "# todo : ë…¸ì¶œìˆœìœ„ ë”°ì§ˆ ë•Œ 1) í•œë²ˆì •ë„ëŠ” ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤í•´ì„œ ê°€ì ¸ì˜¤ê¸° 2) ë„¤ì´ë²„ ë¸”ë¡œê·¸ë„ ìˆœìœ„ì— í¬í•¨ì‹œí‚¤ê¸°.\n",
    "# todo : ê¸€ ìƒì„±ì— ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì €ì¥í•˜ê³  ìœ ì‚¬í‚¤ì›Œë“œ ë¬¸ì§€ë°©ìˆ˜ì¤€  ì¢€ ì¤„ì—¬ì•¼ í•  ê²ƒ ê°™ì•„. ë„ˆë¬´ ê´€ê³„ì—†ëŠ” ì• ë“¤ë„ ê°™ì´ ë¬¶ì´ëŠ” ê²ƒ ê°™ì–´.\n",
    "# todo : ë¸”ë¡œê·¸ì— ì˜¬ë¦´ ë•Œ, ì„±ê³µí•œ ê¸€ì— ëŒ€í•´ì„œë§Œ uploadedë¡œ ì˜®ê¸°ë„ë¡ í•´ì•¼í•  ê²ƒ ê°™ì–´.\n",
    "# todo : suitable checkë¥¼ í•  ë•Œ, ê¸°ì¡´ screened_keywordsì— ìˆëŠ” í‚¤ì›Œë“œë“¤ì€ ì œì™¸í•˜ë„ë¡ í•´ì•¼í•  ê²ƒ ê°™ì•„. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹œì— collected_keywordsì—ë‹¤ê°€ suitability_checked ì—¬ë¶€ë¥¼ í‘œì‹œë¥¼ í•´ë‘ì–´ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê¸°ë³¸ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ì ì¸ í™˜ê²½ì„¤ì •ì„ í•©ë‹ˆë‹¤.\n",
    "isHeadless = True\n",
    "verbose = True\n",
    "\n",
    "\n",
    "from module.Crawler import Crawler\n",
    "from module.File_manager import File_manager\n",
    "from module.Blogger import Blogger\n",
    "from module.AIAgent import AIAgent\n",
    "from module.Uploader import Uploader\n",
    "\n",
    "# ê²½ê³ ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ì €ì¥ (.env) í™•ì¸\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "verbose = True\n",
    "isHeadless = True\n",
    "\n",
    "# # testerìš© ê°ì²´ë“¤ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "# verbose = True\n",
    "# isHeadless = True\n",
    "# crawler_tester = Crawler(isHeadless = isHeadless, verbose = verbose)\n",
    "# file_manager_tester = File_manager(blogname='tester', verbose=verbose)\n",
    "# aiagent_tester = AIAgent(blogname='tester', verbose = verbose)\n",
    "# uploader_tester = Uploader(blogname='tester', verbose = verbose)\n",
    "# blogger_tester = Blogger(blogname='tester', verbose = verbose, isHeadless = isHeadless)\n",
    "\n",
    "def thread_bloggers(func, bloggers, **kwargs):\n",
    "    import threading\n",
    "\n",
    "    threads = []\n",
    "    for blogger in bloggers.values():\n",
    "        # args íŠœí”Œì— bloggerë¥¼ í¬í•¨ì‹œí‚¤ê³ , kwargs ë”•ì…”ë„ˆë¦¬ë¥¼ threading.Threadì— ì „ë‹¬\n",
    "        thread = threading.Thread(target=func, args=(blogger,), kwargs=kwargs)\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "def create_partial_bloggers(total_bloggers:dict, selected_bloggers:list) :\n",
    "  partial_bloggers = {}\n",
    "  for blogger in selected_bloggers :\n",
    "    partial_bloggers[blogger] = total_bloggers[blogger]\n",
    "  return partial_bloggers\n",
    "\n",
    "\n",
    "\n",
    "# blogger í´ë˜ìŠ¤ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "from module.Blogger import Blogger\n",
    "blog_names = ['statifi',\n",
    " 'kindmom', 'sweetkiwi', 'takuz', 'morningbbobbo'\n",
    " ]\n",
    "bloggers = {}\n",
    "for blogname in blog_names:\n",
    "    print(f'ğŸŒ now loading : {blogname}.')\n",
    "    bloggers[blogname] = Blogger(blogname = blogname, verbose = verbose, isHeadless = isHeadless)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.AIAgent import AIAgent\n",
    "tester = AIAgent(blogname='tester', verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.vectorstore_save_texts('ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.vectorstore.delete([_id])\n",
    "# tester.vectorstore_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m new_keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(data) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(stored_keywords))\n\u001b[0;32m      4\u001b[0m new_keywords\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore_save_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\hellodear\\tistory_ebook\\module\\AIAgent.py:104\u001b[0m, in \u001b[0;36mAIAgent.vectorstore_save_texts\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    101\u001b[0m stored_keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore_list[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[0;32m    102\u001b[0m new_keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(data) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(stored_keywords))\n\u001b[1;32m--> 104\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_keywords\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# ë²¡í„°ìŠ¤í† ì–´ì— í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore_list \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore_list, pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: new_keywords, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m : ids})], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# ë²¡í„°ìŠ¤í† ì–´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_local_vectorstore() \u001b[38;5;66;03m# ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\hellodear\\tistory_ebook\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:228\u001b[0m, in \u001b[0;36mFAISS.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    227\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_documents(texts)\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\hellodear\\tistory_ebook\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:199\u001b[0m, in \u001b[0;36mFAISS.__add\u001b[1;34m(self, texts, embeddings, metadatas, ids)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_L2:\n\u001b[0;32m    198\u001b[0m     faiss\u001b[38;5;241m.\u001b[39mnormalize_L2(vector)\n\u001b[1;32m--> 199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Add information to docstore and index.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m ids \u001b[38;5;241m=\u001b[39m ids \u001b[38;5;129;01mor\u001b[39;00m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[1;32mc:\\hellodear\\tistory_ebook\\venv\\Lib\\site-packages\\faiss\\class_wrappers.py:227\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_add\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplacement_add\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Adds vectors to the index.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m    The index must be trained before vectors can be added to it.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m    The vectors are implicitly numbered in sequence. When `n` vectors are\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m        `dtype` must be float32.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m     n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n\u001b[0;32m    229\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "stored_keywords = tester.vectorstore_list['data'].to_list()\n",
    "data = ['ì¢‹ì€ì•„ì¹¨1','ì¢‹ì€ì•„ì¹¨2','ì¢‹ì€ì•„ì¹¨3']\n",
    "new_keywords = list(set(data) - set(stored_keywords))\n",
    "new_keywords\n",
    "\n",
    "tester.vectorstore_save_texts([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.vectorstore_save_texts(['ì¢‹ì€ì•„ì¹¨','ì¢‹ì€ì•„ì¹¨','ì¢‹ì€ì•„ì¹¨','ì¢‹ì€ì•„ì¹¨','ì¢‹ì€ì•„ì¹¨','ì¢‹ì€ì•„ì¹¨','ì¢‹ì€ì•„ì¹¨3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "[Errno 2] No such file or directory: 'c:\\\\hellodear\\\\tistory_ebook\\\\bloggers\\\\tester\\\\storage\\\\[[tester]]_((ko))_``collected_keywords``.json'\n",
      "í•´ë‹¹ ë¸”ë¡œê·¸ëŠ” ìˆ˜ì§‘í•œ í‚¤ì›Œë“œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from module.Blogger import Blogger\n",
    "tester = Blogger(blogname='tester')\n",
    "documents, hrefs = tester.crawler.ddgsearch_reducing('íŒŒì´ì–´ì¡±')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>íŒŒì´ì–´ì¡± - Daum ë°±ê³¼\\në³¸ë¬¸ ë°”ë¡œê°€ê¸°\\në©”ë‰´ ë°”ë¡œê°€ê¸°\\në°±ê³¼ì‚¬ì „\\nê²€ìƒ‰\\nê²€ìƒ‰...</td>\n",
       "      <td>9f0a44b2-2d57-4137-8be4-6ad5cba3e5eb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>íŒŒì´ì–´ì¡± ëœ» ì˜ë¯¸ ì„¤ëª…\\në³¸ë¬¸ ë°”ë¡œê°€ê¸°\\n              \\n       ...</td>\n",
       "      <td>e14dafea-4883-4e5a-abb4-99f445f0339d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ï¿½ï¿½ï¿½ï¿½Å°\\nHOME\\nTOP\\nBOTTOM\\nÂ \\n \\nï¿½ï¿½ï¿½ï¿½\\nï¿½Ë»ï¿½\\nï¿½Ë»Ñ°...</td>\n",
       "      <td>89180dbb-e8c9-483e-a0c0-c0022f18013c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0\\n 17\\n [* ï¿½Í¸ï¿½1 *]\\nï¿½ï¿½ï¿½Ì¾ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ...</td>\n",
       "      <td>c399cd3f-b31b-4884-a819-c404c15543d4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-23\\nï¿½ï¿½ï¿½ï¿½Ö¼Òºï¿½ï¿½ï¿½\\n 1\\n 4\\n [* ï¿½Í¸ï¿½10 *]\\nï¿½...</td>\n",
       "      <td>0fc27ac4-f359-490e-b7d1-13af1e28dd34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>íŒŒì´ì–´ì¡± ì‚¬ë¡€Â \\nPete Adeney\\nPete AdeneyëŠ” Mr.Money M...</td>\n",
       "      <td>4ba4522d-7d9a-4274-8228-cf617f82b2b3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1. ì €ì¶•, íˆ¬ì, ì†Œë¹„ì˜ ê· í˜•ì„ ë§ì¶˜ë‹¤.Â \\nÂ - ì €ì¶•ê³¼ ì†Œë¹„ ì ˆì•½ìœ¼ë¡œë§Œ ì¡°ê¸° ì€...</td>\n",
       "      <td>6920e9a7-f231-4dd8-b2e3-70580445e45d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>ì§ ëŒì´ ë…ì„œ ì¼ê¸° \\n ê¼¬ë¦¬ì— ê¼¬ë¦¬ë¥¼ ë¬´ëŠ” ë…ì„œ \\n ì§ ëŒì´ì˜ ê²½ì œ ê³µë¶€ \\n ê³µì§œ...</td>\n",
       "      <td>9b57e8e8-ed01-4571-8369-6534cd51f0ff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>ï¿½Î»ï¿½ Ä®ï¿½Î¸ï¿½\\n ï¿½ï¿½ï¿½Ë¸ï¿½ï¿½å¸®ï¿½ï¿½\\n ï¿½ß°ï¿½È£\\n Æ®ï¿½ï¿½ï¿½Ã¿ï¿½ï¿½ï¿½\\n ï¿½Ú¿ï¿½ï¿½ï¿½...</td>\n",
       "      <td>d863b557-63e2-4e4c-ad47-0a8c1eb35436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>ê²Œë“ Â ë™ì›í•´ì„œë¼ë„Â ì§‘ì¤‘Â íˆ¬ìë¥¼Â í•´ì•¼ë§ŒÂ ì§ì„±ì´Â í’€ë¦½ë‹ˆë‹¤.Â íˆ¬ìì—ì„œÂ ì–»ì€Â ì„±ì·¨ì—Â ë§Œì¡±ê°...</td>\n",
       "      <td>3f8dc4f8-bd6b-4dd0-b02c-e1b289f97710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  data                                    id\n",
       "0    íŒŒì´ì–´ì¡± - Daum ë°±ê³¼\\në³¸ë¬¸ ë°”ë¡œê°€ê¸°\\në©”ë‰´ ë°”ë¡œê°€ê¸°\\në°±ê³¼ì‚¬ì „\\nê²€ìƒ‰\\nê²€ìƒ‰...  9f0a44b2-2d57-4137-8be4-6ad5cba3e5eb\n",
       "1    íŒŒì´ì–´ì¡± ëœ» ì˜ë¯¸ ì„¤ëª…\\në³¸ë¬¸ ë°”ë¡œê°€ê¸°\\n              \\n       ...  e14dafea-4883-4e5a-abb4-99f445f0339d\n",
       "2    ï¿½ï¿½ï¿½ï¿½Å°\\nHOME\\nTOP\\nBOTTOM\\nÂ \\n \\nï¿½ï¿½ï¿½ï¿½\\nï¿½Ë»ï¿½\\nï¿½Ë»Ñ°...  89180dbb-e8c9-483e-a0c0-c0022f18013c\n",
       "3    0\\n 17\\n [* ï¿½Í¸ï¿½1 *]\\nï¿½ï¿½ï¿½Ì¾ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ...  c399cd3f-b31b-4884-a819-c404c15543d4\n",
       "4    2024-04-23\\nï¿½ï¿½ï¿½ï¿½Ö¼Òºï¿½ï¿½ï¿½\\n 1\\n 4\\n [* ï¿½Í¸ï¿½10 *]\\nï¿½...  0fc27ac4-f359-490e-b7d1-13af1e28dd34\n",
       "..                                                 ...                                   ...\n",
       "148  íŒŒì´ì–´ì¡± ì‚¬ë¡€Â \\nPete Adeney\\nPete AdeneyëŠ” Mr.Money M...  4ba4522d-7d9a-4274-8228-cf617f82b2b3\n",
       "149  1. ì €ì¶•, íˆ¬ì, ì†Œë¹„ì˜ ê· í˜•ì„ ë§ì¶˜ë‹¤.Â \\nÂ - ì €ì¶•ê³¼ ì†Œë¹„ ì ˆì•½ìœ¼ë¡œë§Œ ì¡°ê¸° ì€...  6920e9a7-f231-4dd8-b2e3-70580445e45d\n",
       "150  ì§ ëŒì´ ë…ì„œ ì¼ê¸° \\n ê¼¬ë¦¬ì— ê¼¬ë¦¬ë¥¼ ë¬´ëŠ” ë…ì„œ \\n ì§ ëŒì´ì˜ ê²½ì œ ê³µë¶€ \\n ê³µì§œ...  9b57e8e8-ed01-4571-8369-6534cd51f0ff\n",
       "151  ï¿½Î»ï¿½ Ä®ï¿½Î¸ï¿½\\n ï¿½ï¿½ï¿½Ë¸ï¿½ï¿½å¸®ï¿½ï¿½\\n ï¿½ß°ï¿½È£\\n Æ®ï¿½ï¿½ï¿½Ã¿ï¿½ï¿½ï¿½\\n ï¿½Ú¿ï¿½ï¿½ï¿½...  d863b557-63e2-4e4c-ad47-0a8c1eb35436\n",
       "152  ê²Œë“ Â ë™ì›í•´ì„œë¼ë„Â ì§‘ì¤‘Â íˆ¬ìë¥¼Â í•´ì•¼ë§ŒÂ ì§ì„±ì´Â í’€ë¦½ë‹ˆë‹¤.Â íˆ¬ìì—ì„œÂ ì–»ì€Â ì„±ì·¨ì—Â ë§Œì¡±ê°...  3f8dc4f8-bd6b-4dd0-b02c-e1b289f97710\n",
       "\n",
       "[153 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if tester.verbose : print(f\"ğŸŒ ê¸€ ê´€ë ¨ ìë£Œ ìˆ˜ì§‘ ì¤‘ ... \")\n",
    "tester.posting_ai.results['topics'] = ['íŒŒì´ì–´ì¡±', 'íŒŒì´ì–´ì¡± íŠ¹ì§•', 'íŒŒì´ì–´ì¡± ë˜ëŠ” ë²•']\n",
    "for topic in tester.posting_ai.results['topics'] :\n",
    "    documents, hrefs = tester.crawler.ddgsearch_reducing(topic)    \n",
    "    tester.posting_ai.results['documents_urls'].append(documents)\n",
    "    tester.posting_ai.results['documents'].append(hrefs)\n",
    "\n",
    "# ê¸ì–´ë“¤ì¸ ê¸€ë“¤ì„ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥í•˜ê¸°                \n",
    "collected_documents = []\n",
    "for document in tester.posting_ai.results['documents'] :\n",
    "    collected_documents.extend(document)\n",
    "tester.posting_ai.vectorstore_recursive_save_texts(collected_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "keywords = [document.page_content for document in tester.posting_ai.vectorstore.similarity_search('', k=150)]\n",
    "for keyword in ['keywords'] :\n",
    "    a = tester.posting_ai.vectorstore_list[tester.posting_ai.vectorstore_list['data'] == keyword]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True : \n",
    "    random = tester.posting_ai.vectorstore.similarity_search('', k=1)[0].page_content\n",
    "    print(random)\n",
    "    tester.posting_ai.vectorstore_extract(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester.posting_ai.vectorstore.similarity_search('', k=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í´ë¦­1 : í‚¤ì›Œë“œ ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì“°ë ˆë”©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ì„¤ì •\n",
    "\n",
    "# ê¹Šì´ ì •ì˜\n",
    "depth = int(input('í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘í•  ê¹Šì´ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ìµœëŒ€ 3 ì´ìƒì€ ë¹„ì¶”ì²œ) :'))\n",
    "\n",
    "# ë‹¨ì–´ ì •ì˜\n",
    "recommended = {\n",
    "    \"ê±´ê°•ê³¼ ì›°ë¹™\": [\"ìš”ê°€\", \"ë§ˆì¸ë“œí’€ë‹ˆìŠ¤\", \"ì›°ë‹ˆìŠ¤ ë¦¬íŠ¸ë¦¿\", \"ê±´ê°•í•œ ê°„ì‹\", \"ëª…ìƒ\", \"í•„ë¼í…ŒìŠ¤\", \"ê±´ê°•í•œ ì‹ë‹¨\", \"ê±´ê°• ì¶”ì„¸\", \"íë§\", \"ê±´ê°• ê²€ì§„\"],\n",
    "    \"ê¸°ì—…ê³¼ ê¸°ì—…ê°€\": [\"ê¸°ì—…ê°€ ì •ì‹ \", \"í¬ë¼ìš°ë“œí€ë”©\", \"ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ í˜ì‹ \", \"ê¸°ìˆ  ë²¤ì²˜\", \"ë²¤ì²˜ íˆ¬ì\", \"ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„\"],\n",
    "    \"ì¬íƒ ê·¼ë¬´ ë° ìê¸° ê³„ë°œ ë¸”ë¡œê·¸\": [\"ì¬íƒ ê·¼ë¬´ íŒ\", \"ìê¸°ê³„ë°œ ì±… ì¶”ì²œ\", \"ì˜¨ë¼ì¸ í•™ìŠµ í”Œë«í¼\", \"ì‹œê°„ ê´€ë¦¬ ê¸°ìˆ \", \"ìƒì‚°ì„± í–¥ìƒ\", \"ì‚¬ì´ë“œ í—ˆìŠ¬\", \"ì¬íƒ ê·¼ë¬´ í™˜ê²½\", \"í”„ë¦¬ëœì„œ ê°€ì´ë“œ\", \"ìê¸° ê´€ë¦¬\", \"ëª¨í‹°ë² ì´ì…˜ ì¦ì§„\"],\n",
    "    \"ê±´ê°• ë° ì›°ë‹ˆìŠ¤ ë¸”ë¡œê·¸\": [\"ê±´ê°•í•œ ì‹ë‹¨\", \"ì •ì‹  ê±´ê°•\", \"ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬\", \"í™ˆ í”¼íŠ¸ë‹ˆìŠ¤\", \"ëª…ìƒ ê¸°ë²•\", \"ì›°ë‹ˆìŠ¤ ë¼ì´í”„ìŠ¤íƒ€ì¼\", \"ì˜ì–‘ ì •ë³´\", \"ê±´ê°• ê²€ì§„ ê°€ì´ë“œ\", \"ì²´ì¤‘ ê´€ë¦¬\", \"ê±´ê°•í•œ ìŠµê´€\"],\n",
    "    \n",
    "}\n",
    "\n",
    "subjects_n_words_for_statifi = {}\n",
    "subjects_n_words_for_kindmom = {}\n",
    "subjects_n_words_for_sweetkiwi = {}\n",
    "subjects_n_words_for_takuz = {}\n",
    "subjects_n_words_for_morningbbobbo = {}\n",
    "\n",
    "# ê° ë¸”ë¡œê±°ì˜ ì£¼ì œ ë° í‚¤ì›Œë“œ ë°ì´í„°\n",
    "\n",
    "total_subjects_n_words = {\n",
    "    'statifi': subjects_n_words_for_statifi,\n",
    "    'kindmom': subjects_n_words_for_kindmom,\n",
    "    'sweetkiwi': subjects_n_words_for_sweetkiwi,\n",
    "    'takuz': subjects_n_words_for_takuz,\n",
    "    'morningbbobbo': subjects_n_words_for_morningbbobbo\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# í•¨ìˆ˜ì •ì˜\n",
    "def collect_keywords_for_blogger(blogger, total_subjects_n_words, depth, save):\n",
    "    subjects_n_words = total_subjects_n_words[blogger.blogname]\n",
    "    blogger.collect_keywords(subjects_n_words=subjects_n_words, depth=depth, save=save)\n",
    "\n",
    "\n",
    "# ì‹¤í–‰\n",
    "thread_bloggers(\n",
    "    func=collect_keywords_for_blogger,\n",
    "    bloggers=bloggers,\n",
    "    total_subjects_n_words=total_subjects_n_words,\n",
    "    depth=depth,\n",
    "    save=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tester í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ì¼ ë¸”ë¡œê±°ë¡œ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "test_blog = 'tester'\n",
    "total_subjects_n_words = {test_blog : {'íŒŒì´ì–´ì¡±' : ['íŒŒì´ì–´ì¡±']} }\n",
    "depth = 1\n",
    "\n",
    "# í•¨ìˆ˜ì •ì˜\n",
    "def collect_keywords_for_blogger(blogger, total_subjects_n_words, depth, save):\n",
    "    subjects_n_words = total_subjects_n_words[blogger.blogname]\n",
    "    blogger.collect_keywords(subjects_n_words=subjects_n_words, depth=depth, save=save)\n",
    "\n",
    "\n",
    "collect_keywords_for_blogger(blogger_tester, total_subjects_n_words, depth, save=True)\n",
    "\n",
    "vectorstore_stored_keywords = blogger_tester.keyword_ai.vectorstore_list.index.to_list()\n",
    "if len(vectorstore_stored_keywords) > 0 :\n",
    "    print('ğŸŒ í‚¤ì›Œë“œ ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.')    \n",
    "    blogger_tester.keyword_ai.vectorstore_clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í´ë¦­ 2 : í¬ìŠ¤íŒ… ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìˆœì°¨ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ì„¤ì •\n",
    "# num_contents_creation = int(input('ìƒì„±í•  ì»¨í…ì¸  ê°¯ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ë¬´í•œëŒ€ìƒì‚°ì€ ì—”í„°ë¥¼ ì¹˜ì„¸ìš” '))\n",
    "num_contents_creation = 20\n",
    "\n",
    "# í•¨ìˆ˜ì •ì˜\n",
    "def create_contents(blogger, num_contents_creation=5):\n",
    "    print(f\"Starting thread for {blogger.blogname}\")\n",
    "    blogger.create_contents(num_contents_creation)\n",
    "\n",
    "for blogname in blog_names :\n",
    "    create_contents(bloggers[blogname], num_contents_creation=num_contents_creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in bloggers['statifi'].keyword_ai.vectorstore_list['data'].to_list() :\n",
    "    a = bloggers['statifi'].keyword_ai.vectorstore.similarity_search(i)[0].page_content\n",
    "    if a != i :\n",
    "        print(a, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì“°ë ˆë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ì„¤ì •\n",
    "num_contents_creation = int(input('ìƒì„±í•  ì»¨í…ì¸  ê°¯ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ë¬´í•œëŒ€ìƒì‚°ì€ ì—”í„°ë¥¼ ì¹˜ì„¸ìš” '))\n",
    "\n",
    "# í•¨ìˆ˜ì •ì˜\n",
    "def create_contents(blogger, num_contents_creation=5):\n",
    "    print(f\"Starting thread for {blogger.blogname}\")\n",
    "    blogger.create_contents(num_contents_creation)\n",
    "\n",
    "# ì‹¤í–‰\n",
    "thread_bloggers(create_contents, bloggers, num_contents_creation=num_contents_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¶€ë¶„ì“°ë ˆë”© (2 + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ì„¤ì •\n",
    "# num_contents_creation = int(input('ìƒì„±í•  ì»¨í…ì¸  ê°¯ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ë¬´í•œëŒ€ìƒì‚°ì€ ì—”í„°ë¥¼ ì¹˜ì„¸ìš” '))\n",
    "num_contents_creation = 20  \n",
    "\n",
    "# í•¨ìˆ˜ì •ì˜\n",
    "def create_contents(blogger, num_contents_creation=5):\n",
    "    print(f\"Starting thread for {blogger.blogname}\")\n",
    "    blogger.create_contents(num_contents_creation)\n",
    "\n",
    "partial_bloggers = create_partial_bloggers(bloggers, ['statifi', 'sweetkiwi', 'kindmom'])\n",
    "thread_bloggers(create_contents, partial_bloggers, num_contents_creation=num_contents_creation)\n",
    "partial_bloggers2 = create_partial_bloggers(bloggers, ['takuz', 'morningbbobbo'])\n",
    "thread_bloggers(create_contents, partial_bloggers2, num_contents_creation=num_contents_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tester í…ŒìŠ¤íŠ¸ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ì¼ë¸”ë¡œê±°ë¡œ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "# í•¨ìˆ˜ì •ì˜\n",
    "def create_contents(blogger, num_contents_creation=5):\n",
    "    print(f\"Starting thread for {blogger.blogname}\")\n",
    "    blogger.create_contents(num_contents_creation)\n",
    "\n",
    "# í•¨ìˆ˜ì‹¤í–‰\n",
    "\n",
    "blogger_tester.keyword_ai.vectorstore_save_texts('íŒŒì´ì–´ì¡± íŠ¹ì§•') # ì˜ˆì‹œ í‚¤ì›Œë“œ ì €ì¥\n",
    "num_of_created_contents_now = len(blogger_tester.file_manager.get_file_names())\n",
    "\n",
    "try :\n",
    "    create_contents(blogger_tester, num_contents_creation=1)\n",
    "finally :\n",
    "    blogger_tester.keyword_ai.vectorstore_clear()\n",
    "\n",
    "# ì‹¤í–‰ê²°ê³¼ í™•ì¸\n",
    "num_of_created_contents_after = len(blogger_tester.file_manager.get_file_names())\n",
    "if (num_of_created_contents_after - num_of_created_contents_now) > 0 :\n",
    "    print('ğŸŒ ì»¨í…ì¸  ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í´ë¦­3 : í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìˆœì°¨ë°˜ë³µë¬¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ì„¤ì •\n",
    "num_contents_upload = int(input('ì—…ë¡œë“œ í•  ì»¨í…ì¸  ê°¯ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ë¬´í•œëŒ€ìƒì‚°ì€ ì—”í„°ë¥¼ ì¹˜ì„¸ìš”. '))\n",
    "assert num_contents_upload is not None, 'ì—…ë¡œë“œí•  ì»¨í…ì¸  ê°¯ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.'\n",
    "assert num_contents_upload <= 15 , '15ê°œ ì´í•˜ë¡œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.'\n",
    "\n",
    "# í•¨ìˆ˜ì •ì˜\n",
    "def upload(blogger, num_contents_upload):\n",
    "    blogger.upload_contents(num_contents_upload)\n",
    "\n",
    "for blogname in blog_names :\n",
    "    upload(bloggers[blogname], num_contents_upload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì“°ë ˆë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ì„¤ì •\n",
    "num_contents_upload = int(input('ì—…ë¡œë“œ í•  ì»¨í…ì¸  ê°¯ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ë¬´í•œëŒ€ìƒì‚°ì€ ì—”í„°ë¥¼ ì¹˜ì„¸ìš”. '))\n",
    "assert num_contents_upload is not None, 'ì—…ë¡œë“œí•  ì»¨í…ì¸  ê°¯ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.'\n",
    "assert num_contents_upload <= 15 , '15ê°œ ì´í•˜ë¡œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.'\n",
    "\n",
    "# í•¨ìˆ˜ì •ì˜\n",
    "def upload(blogger, num_contents_upload):\n",
    "    blogger.upload_contents(num_contents_upload)\n",
    "\n",
    "# ì‹¤í–‰\n",
    "thread_bloggers(\n",
    "    func=upload,\n",
    "    bloggers=bloggers,\n",
    "    num_contents_upload=num_contents_upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í…ŒìŠ¤íŠ¸ì½”ë“œ (blogger_tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•¨ìˆ˜ì •ì˜\n",
    "def upload(blogger, num_contents_upload = 1):\n",
    "    blogger.upload_contents(num_contents_upload)\n",
    "\n",
    "# í•¨ìˆ˜ì‹¤í–‰\n",
    "\n",
    "os.environ[\"tester_NEW_POST_URL\"] = str(r\"https://statifi.tistory.com/manage/newpost/?type=post&returnURL=%2Fmanage%2Fposts%2F\")\n",
    "os.environ[\"tester_ID\"] = \"jsj950611@naver.com\"\n",
    "os.environ[\"tester_PW\"] = \"Zhzkzhffk1!\"\n",
    "\n",
    "\n",
    "upload(blogger_tester, num_contents_upload=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í…ŒìŠ¤íŠ¸ì½”ë“œëŠ” ì—¬ê¸° ì´í•˜ì— ì‘ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚¨ì€ ê¸€ì˜ ê°¯ìˆ˜\n",
    "for blogname in blog_names:\n",
    "    print(len(bloggers[blogname].file_manager.get_file_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚¨ì€ í‚¤ì›Œë“œì˜ ê°¯ìˆ˜\n",
    "for blogname in blog_names:\n",
    "    print(len(bloggers[blogname].keyword_ai.vectorstore_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë¦„ìˆœì„œ\n",
    "for blogname in blog_names:\n",
    "    print(blogname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bloggers['takuz'].file_manager.get_keywords('collected_keywords')\n",
    "isNaverIn = []\n",
    "for all_links in df['all_links'] :\n",
    "    done = False\n",
    "    for link in all_links :        \n",
    "        if link is None : \n",
    "            isNaverIn.append(False)\n",
    "            done = True\n",
    "            break\n",
    "        if 'naver' in link :\n",
    "            isNaverIn.append(True)\n",
    "            done = True\n",
    "            break\n",
    "    if done == False :\n",
    "        isNaverIn.append(False)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isNaverIn'] = isNaverIn\n",
    "len(df[df['isNaverIn'] == True][df['tistory_rank_at_google'] == 'ë…¸ì¶œì—†ìŒ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.Crawler import Crawler\n",
    "crawler = Crawler(isHeadless=False)\n",
    "crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.search('íŒŒì´ì–´ì¡±', 'google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# ì›¹í˜ì´ì§€ URL\n",
    "url = crawler.createURL('íŒŒì´ì–´ì¡±', 'daum')\n",
    "url\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "\n",
    "# BeautifulSoup ê°ì²´ ìƒì„±\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "soup\n",
    "# idê°€ 'search'ì¸ ìš”ì†Œ ì°¾ê¸°\n",
    "items = soup.find_all('div', class_='c-item-doc')\n",
    "items\n",
    "\n",
    "# ê° ìš”ì†Œ ë‚´ì˜ ëª¨ë“  ë§í¬ ì¶”ì¶œ\n",
    "for item in items:\n",
    "    links = [a['href'] for a in item.find_all('a', href=True)]\n",
    "    print(links)\n",
    "\n",
    "# # í•´ë‹¹ ìš”ì†Œ ë‚´ ëª¨ë“  ë§í¬ ì¶”ì¶œ\n",
    "# links = [a['href'] for a in search_body.find_all('a', href=True)]\n",
    "\n",
    "# print(links)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.search('íŒŒì´ì–´ì¡±', 'daum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def\n",
    "selector = \"c-doc-web > div > div.item-title > c-title > strong > a\"\n",
    "crawler.selenium_scroll_action()\n",
    "crawler.selenium_crawling(selector, get_attribute='href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddgsearch_reducing(self, keyword, how_many_retrieve=5):\n",
    "        \"\"\"ì›¹ë¸Œë¼ìš°ì € ë•ë•ê³ ë¡œë¶€í„° í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "        ê·¸ ë‹¤ìŒ í•´ë‹¹ ë§í¬ë“¤ë¡œë¶€í„° ë³¸ë¬¸ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ìˆ˜ì§‘í•œ ìë£ŒëŠ” ì´í›„ì— RAGì— í™œìš©í•©ë‹ˆë‹¤.\"\"\"\n",
    "        urls, contents = [], [] # ì‚¬ì´íŠ¸ ì£¼ì†Œì™€ ë‚´ìš©ì„ ë‹´ì„ ì»¨í…Œì´ë„ˆë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "        try:\n",
    "            selector = \"c-doc-web > div > div.item-title > c-title > strong > a\"\n",
    "            self.search(keyword, 'daum')\n",
    "            urls = crawler.selenium_crawling(selector, get_attribute='href')\n",
    "            # results = DDGS().text(keyword, max_results=20) # ë•ë•ê³ ì—ì„œ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "            # urls = [a['href'] for a in results if 'html' not in a['href'] and a['href'] != '']\n",
    "        except Exception as e:\n",
    "            print(f'{e} with {keyword}')\n",
    "\n",
    "        \n",
    "        try:\n",
    "            idx, clear = 0, 0 # í…ìŠ¤íŠ¸ ìˆ˜ì§‘ì— ì„±ê³µí•œ ê°¯ìˆ˜ë¥¼ ë‹´ì„ ì»¨í…Œì´ë„ˆ ì…ë‹ˆë‹¤.\n",
    "            while clear < how_many_retrieve and idx < len(urls):\n",
    "                url = urls[idx]\n",
    "                content = self.get_text_from_webpage(url, self.driver) # í•´ë‹¹ ë§í¬ë¡œë¶€í„° ë³¸ë¬¸ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "                if content:\n",
    "                    contents.append(self.clean_blog_content(content)) # ìˆ˜ì§‘í•œ ë³¸ë¬¸ì„ ì •ì œí•©ë‹ˆë‹¤.\n",
    "                    clear += 1\n",
    "                idx += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return urls[:how_many_retrieve], contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# ì›¹í˜ì´ì§€ URL\n",
    "url = crawler.createURL('íŒŒì´ì–´ì¡±', 'daum')\n",
    "url\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "\n",
    "# BeautifulSoup ê°ì²´ ìƒì„±\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "soup\n",
    "# idê°€ 'search'ì¸ ìš”ì†Œ ì°¾ê¸°\n",
    "items = soup.find_all('div', class_='c-item-doc')\n",
    "items\n",
    "\n",
    "# ê° ìš”ì†Œ ë‚´ì˜ ëª¨ë“  ë§í¬ ì¶”ì¶œ\n",
    "for item in items:\n",
    "    links = [a['href'] for a in item.find_all('a', href=True)]\n",
    "    print(links)\n",
    "\n",
    "# # í•´ë‹¹ ìš”ì†Œ ë‚´ ëª¨ë“  ë§í¬ ì¶”ì¶œ\n",
    "# links = [a['href'] for a in search_body.find_all('a', href=True)]\n",
    "\n",
    "# print(links)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
