{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# ì •ì˜í•œ í´ë˜ìŠ¤ë“¤ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "from module.Crawler import Crawler\n",
    "from module.AIAgent import AIAgent\n",
    "from module.File_manager import File_manager\n",
    "from module.Uploader import Uploader\n",
    "import copy\n",
    "\n",
    "class Blogger() :\n",
    "    def __init__(self, blogname, verbose = False, isHeadless = True) :         \n",
    "        self.blogname = blogname # ë¸”ë¡œê·¸ ì´ë¦„\n",
    "        self.verbose = verbose # ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€\n",
    "        \"\"\" ë°˜ë“œì‹œ íŒŒì¼ë§¤ë‹ˆì €ë¶€í„° í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤. íŒŒì¼ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ê³¼ í•¨ê»˜ ëª¨ë“  í´ë”êµ¬ì¡°ê°€ ìƒì„±ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\"\"\"\n",
    "        # í´ë˜ìŠ¤ í˜¸ì¶œ        \n",
    "        self.file_manager = File_manager(blogname = blogname, verbose=verbose) # íŒŒì¼ ë§¤ë‹ˆì €\n",
    "        self.crawler = Crawler(verbose=verbose, isHeadless=isHeadless) # í¬ë¡¤ëŸ¬\n",
    "        self.posting_ai = AIAgent(index=\"my_contents\", blogname=blogname  ,verbose=verbose) # í¬ìŠ¤íŒ… AI\n",
    "        self.keyword_ai = AIAgent(index = \"my_keywords\", blogname=blogname, verbose=verbose) # í‚¤ì›Œë“œ AI\n",
    "        self.uploader = Uploader(\n",
    "            blogname=blogname, # ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸, í¬ìŠ¤íŒ…URLì„ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.        \n",
    "            verbose=verbose,\n",
    "            isHeadless = isHeadless\n",
    "            )\n",
    "        \n",
    "        # ê¸°ì¡´ì— ìˆ˜ì§‘í•˜ì—¬ì„œ ì €ì¥í•´ë‘ì—ˆë˜ í‚¤ì›Œë“œë“¤ì´ ìˆìœ¼ë©´ì€ ë¶ˆëŸ¬ì™€ì„œ crawler.resultsì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        # ê·¸ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê³¼ì •ì—ì„œ ì¤‘ë³µë˜ëŠ” í‚¤ì›Œë“œë¥¼ ì œê±°í•  ìˆ˜ ìˆì„ ë¿ë§Œ ì•„ë‹ˆë¼ ìƒˆë¡­ê²Œ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œê°€ ê¸°ì¡´ í‚¤ì›Œë“œì— ë®ì–´ì”Œì›Œì§€ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "        collected_keywords_info = self.file_manager.get_keywords(what_keywords='collected_keywords') # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´            \n",
    "        if collected_keywords_info is not None : # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ê°€ ìˆìœ¼ë©´\n",
    "            self.crawler.results = copy.deepcopy(collected_keywords_info.T.to_dict()) # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ë¥¼ crawler.resultsì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        else :            \n",
    "            print('í•´ë‹¹ ë¸”ë¡œê·¸ëŠ” ìˆ˜ì§‘í•œ í‚¤ì›Œë“œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')        \n",
    "        \n",
    "        # self.collected_keywords_info = self.file_manager.get_keywords(what_keywords='collected_keywords')\n",
    "        # self.collected_keywords = self.collected_keywords_info.keys().to_list()\n",
    "        # self.screened_keywords_info = self.file_manager.get_keywords(what_keywords='collected_keywords')\n",
    "        # self.screened_keywords = self.screened_keywords_info.keys().to_list()\n",
    "        # self.suitable_keywords_info = self.file_manager.get_keywords(what_keywords='collected_keywords')\n",
    "        # self.suitable_keywords = self.suitable_keywords_info.keys().to_list()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def collect_keywords(self, subjects_n_words, depth, save=True) :\n",
    "        self.crawler.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "        try :\n",
    "            # í‚¤ì›Œë“œ ìˆ˜ì§‘ì—ëŠ” crawler í´ë˜ìŠ¤ ë‚´ì—ì„œ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆëŠ” ë°˜ë³µë¬¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "            self.crawler.iterate_keyword_crawling_w_multiple_subjects(depth = depth, subjects_n_words=subjects_n_words, save=save) \n",
    "        except Exception as e :\n",
    "            print(e)\n",
    "        finally : # ì¤‘ë‹¨í•˜ë”ë¼ë„ í˜„ì¬ì§„í–‰ì‹œì ê¹Œì§€ê°€ ë¡œì»¬ì— ì €ì¥ë©ë‹ˆë‹¤.\n",
    "            \n",
    "            # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ ì¤‘ì—ì„œ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ í‚¤ì›Œë“œëŠ” ì œì™¸í•˜ê³  ì €ì¥í•œë‹¤.\n",
    "            # ê·¸ëŸ°ë° ì´ ê¸°ëŠ¥ì€ ì—¬ê¸°ì— ë“¤ì–´ê°ˆ ê²ƒì´ ì•„ë‹ˆë¼ ê¸°ë³¸ì ì¸ Cralwerì˜ iteration í•¨ìˆ˜ì— ë“¤ì–´ê°€ì•¼ í•œë‹¤.\n",
    "            self.crawler.results =  self.crawler.load_results().dropna(\n",
    "                subset = ['num_ads', 'tistory_rank_at_google']).T.to_dict() \n",
    "            print('after interuption : good')\n",
    "\n",
    "            # collected_keywords\n",
    "            collected_keywords_info = self.crawler.load_results() # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´\n",
    "            collected_keywords = self.crawler.get_keywords() # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸\n",
    "            self.file_manager.save_keywords('collected_keywords', self.crawler.results) # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´ ì €ì¥\n",
    "            print('after interuption : collected_keywords = good')\n",
    "\n",
    "\n",
    "            # í‚¤ì›Œë“œ ì„ ë³„\n",
    "            screened_keywords_info = self.crawler.load_processed_results() # ì„ ë³„í•œ í‚¤ì›Œë“œ ì •ë³´\n",
    "            screened_keywords = list(screened_keywords_info.index) # ì„ ë³„í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸\n",
    "            self.file_manager.save_keywords('screened_keywords',  self.crawler.results ) # ì„ ë³„í•œ í‚¤ì›Œë“œ ì •ë³´ ì €ì¥\n",
    "            print('after interuption : screened_keywords = good')\n",
    "\n",
    "            # í‚¤ì›Œë“œ ì ì •ì„± ê²€ì‚¬\n",
    "            subjects = screened_keywords_info.loc[:,'subject'].to_list() # í‚¤ì›Œë“œ ì ì •ì„± ê²€ì‚¬ë¥¼ ìœ„í•œ ì£¼ì œ ë¦¬ìŠ¤íŠ¸            \n",
    "            suitable_keywords = self.keyword_ai.suitability_checker(subjects, screened_keywords) # ì ì •ì„± ê²€ì‚¬\n",
    "            suitable_keywords_info = screened_keywords_info.loc[suitable_keywords] # ì ì •ì„± ê²€ì‚¬ ê²°ê³¼\n",
    "            print('after interuption : good3')\n",
    "            \n",
    "            # í´ë˜ìŠ¤ ë‚´ë¶€ ë³€ìˆ˜ë¡œ ì €ì¥\n",
    "            self.collected_keywords_info = collected_keywords_info # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ì •ë³´\n",
    "            self.collected_keywords = collected_keywords # ìˆ˜ì§‘í•œ ì „ì²´í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸\n",
    "            self.screened_keywords_info = screened_keywords_info # ì„ ë³„í•œ í‚¤ì›Œë“œ ì •ë³´\n",
    "            self.screened_keywords = screened_keywords# ì„ ë³„í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸\n",
    "            self.suitable_keywords_info = suitable_keywords_info # ì ì •í•œ í‚¤ì›Œë“œ ì •ë³´\n",
    "            self.suitable_keywords = suitable_keywords # ì ì •í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸\n",
    "            print('after interuption : good4')\n",
    "            \n",
    "            # ìˆ˜ì§‘í•œ ì „ì²´ í‚¤ì›Œë“œ ë¡œì»¬í™˜ê²½ì— csvíŒŒì¼ë¡œ ì €ì¥            \n",
    "            \n",
    "            self.file_manager.save_keywords('suitable_keywords', suitable_keywords_info.T.to_dict() ) # ì ì •í•œ í‚¤ì›Œë“œ ì •ë³´ ì €ì¥\n",
    "            \n",
    "            \n",
    "            # í‚¤ì›Œë“œ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ (keyword_ai)            \n",
    "            self.keyword_ai.vectorstore_save_texts(suitable_keywords)\n",
    "\n",
    "    def create_contents(self, \n",
    "                        num_contents_creation = None, \n",
    "                        keyword_score_threshold=0.55,\n",
    "                        kewyord_retriever_k = 16,\n",
    "                        num_topic = 3,\n",
    "                        contents_lan = 'English',\n",
    "                        num_images = 1,\n",
    "                        documents_score_threshold=0.25,\n",
    "                        documents_retriever_k = 10) :        \n",
    "\n",
    "        iteration = 0 # ë°˜ë³µíšŸìˆ˜\n",
    "        # ë¬´í•œë°˜ë³µ (ë°˜ë³µíšŸìˆ˜ ë¯¸ì§€ì • ì‹œ ë²¡í„°ìŠ¤í† ì–´ ë‚´ì— í‚¤ì›Œë“œê°€ ë‚¨ì§€ ì•Šì„ ë•Œê¹Œì§€ ë¬´í•œë°˜ë³µí•œë‹¤. ë°˜ë³µíšŸìˆ˜ ì§€ì • ì‹œ í•´ë‹¹ íšŸìˆ˜ë§Œí¼ ë°˜ë³µí•˜ê³  ì¢…ë£Œí•œë‹¤.)\n",
    "        while True : \n",
    "            iteration += 1 # ë°˜ë³µíšŸìˆ˜ 1ì¦ê°€\n",
    "            random_keywords = self.keyword_ai.vectorstore.similarity_search('')\n",
    "            if len(random_keywords) == 0 : # ë²¡í„°ìŠ¤í† ì–´ ë‚´ì— í‚¤ì›Œë“œê°€ ë‚¨ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ\n",
    "                print('ğŸŸ¡ ë²¡í„°ìŠ¤í† ì–´ ë‚´ì— ì €ì¥ëœ ëª¨ë“  í‚¤ì›Œë“œë¥¼ ì†Œì§„í•˜ì˜€ìŠµë‹ˆë‹¤. ê³ ìƒí•˜ì…¨ìŠµë‹ˆë‹¤.')\n",
    "                break\n",
    "            try : \n",
    "                my_keyword = self.keyword_ai.vectorstore.similarity_search('')[0].page_content # ê¸€ì„ ì“¸ ì²«ë²ˆì§¸ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.    \n",
    "                my_keywords = self.keyword_ai.vectorstore_extract(\n",
    "                    my_keyword, \n",
    "                    score_threshold=keyword_score_threshold,\n",
    "                    k = kewyord_retriever_k\n",
    "                    )     # ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë²¡í„°ìŠ¤í† ì–´ ë‚´ì—ì„œ ì‚­ì œ (extract)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                self.posting_ai.create_topics(my_keywords, num_topic=num_topic, save=True)     # ì†Œì œëª© 3ê°œ ìƒì„±\n",
    "                self.posting_ai.create_prologue(self.posting_ai.results['topics'], self.posting_ai.results['keywords'], save=True)     # í”„ë¡¤ë¡œê·¸ ìƒì„±\n",
    "                self.posting_ai.create_title(self.posting_ai.results['topics'], self.posting_ai.results['keywords'], save=True)     # ì œëª© ìƒì„±\n",
    "                # ìë£Œìˆ˜ì§‘\n",
    "                for topic in self.posting_ai.results['topics'] :\n",
    "                    documents, hrefs = self.crawler.ddgsearch_reducing(topic)    \n",
    "                    self.posting_ai.results['documents_urls'].append(documents)\n",
    "                    self.posting_ai.results['documents'].append(hrefs)\n",
    "\n",
    "                # ê¸ì–´ë“¤ì¸ ê¸€ë“¤ì„ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥í•˜ê¸°\n",
    "                collected_documents = []\n",
    "                for document in self.posting_ai.results['documents'] :\n",
    "                    collected_documents.extend(document)\n",
    "                self.posting_ai.vectorstore_recursive_save_texts(collected_documents)\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                # ê¸€ ì‘ì„±\n",
    "                self.posting_ai.create_content(\n",
    "                    topics = self.posting_ai.results['topics'], \n",
    "                    language=contents_lan, \n",
    "                    score_threshold=documents_score_threshold,  \n",
    "                    k=documents_retriever_k,  \n",
    "                    save=True)\n",
    "\n",
    "                # ì´ë¯¸ì§€ ìˆ˜ì§‘í•˜ê¸°    \n",
    "                topics = self.posting_ai.results['topics']\n",
    "                for topic in topics : \n",
    "                    images = self.crawler.ddgsearch_get_images(topic, max_results = num_images)\n",
    "                    self.posting_ai.results['images'].append(images)    \n",
    "\n",
    "                # ê¸€ í¬ë§·íŒ…í•˜ê¸°\n",
    "                self.posting_ai.create_HTML_formmater(save=True)\n",
    "\n",
    "                # ê²°ê³¼ì €ì¥í•˜ê¸°\n",
    "\n",
    "                # ìˆ˜ì§‘ ë° ìƒì„±í•œ ëª¨ë“  ìë£Œë¥¼ jsonìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "                self.file_manager.saveGeneratedDict2Json(\n",
    "                    data = self.posting_ai.results,\n",
    "                    subject = self.blogname,\n",
    "                    language = 'ko',\n",
    "                    contents = self.posting_ai.results['title'],\n",
    "                    folder_category='storage'\n",
    "                )\n",
    "                # í¬ìŠ¤íŒ…í•  ë¬¸ì„œë¥¼ txtë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "                self.file_manager.saveGeneratedText2Text(\n",
    "                    data = self.posting_ai.results['html_for_upload'],\n",
    "                    subject = self.blogname,\n",
    "                    language = 'ko',\n",
    "                    contents = self.posting_ai.results['title'],\n",
    "                    folder_category='for_upload'\n",
    "            )\n",
    "                # ì €ì¥ í›„ self.posting_aiì˜ results ì´ˆê¸°í™”.\n",
    "                self.posting_ai.clear()\n",
    "            except Exception as e :\n",
    "                print(e)\n",
    "                continue\n",
    "            finally :\n",
    "                if num_contents_creation is not None :\n",
    "                    if iteration >= num_contents_creation :\n",
    "                        break\n",
    "\n",
    "\n",
    "    def upload_contents(self, num_contents_upload = 15) :\n",
    "\n",
    "        assert len(os.getenv(f'{self.blogname}_ID')) > 0, 'ë¸”ë¡œê·¸ ì•„ì´ë””ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•˜ì„¸ìš”.'\n",
    "        assert len(os.getenv(f'{self.blogname}_PW')) > 0, 'ë¸”ë¡œê·¸ ë¹„ë°€ë²ˆí˜¸ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•˜ì„¸ìš”.'\n",
    "        assert len(os.getenv(f'{self.blogname}_NEW_POST_URL')) > 0, 'ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…URLë¥¼ í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•˜ì„¸ìš”.'\n",
    "\n",
    "        self.uploader.is_selenium_turned_on() # ì…€ë ˆë‹ˆì›€ì´ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "        # ì—…ë¡œë“œ í•  ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "        filenames = self.file_manager.get_file_names() # ì „ì²´ íŒŒì¼ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "\n",
    "        upload_titles = [] # ì—…ë¡œë“œí•  ë¬¸ì„œì˜ ì œëª© ì»¨í…Œì´ë„ˆ\n",
    "        upload_contents = [] # ì—…ë¡œë“œí•  ë¬¸ì„œì˜ ë‚´ìš© ì»¨í…Œì´ë„ˆ\n",
    "        \n",
    "        for filename in filenames[:num_contents_upload] :\n",
    "            _, _, title, _ = self.file_manager.extract_elements_from_filename(filename) # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ\n",
    "            text = self.file_manager.loadText2Text_w_filename(filename, 'for_upload') # íŒŒì¼ ë‚´ìš© ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "            upload_titles.append(title) # ì»¨í…Œì´ë„ˆì— ì œëª© ì¶”ê°€\n",
    "            upload_contents.append(text)   # ì»¨í…Œì´ë„ˆì— ë‚´ìš© ì¶”ê°€\n",
    "            self.file_manager.moveComplete(filename) # ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œì´ ì™„ë£Œëœ ì™„ë£Œëœ íŒŒì¼ì€ ì´ë™í•©ë‹ˆë‹¤.\n",
    "\n",
    "        if self.verbose : print(f\"ì—…ë¡œë“œ í•  ë¬¸ì„œì˜ ê°¯ìˆ˜ëŠ” {len(upload_contents)}ê°œ ì…ë‹ˆë‹¤.\")\n",
    "        \n",
    "        try :\n",
    "            self.uploader.tistory_upload(\n",
    "                titles = upload_titles, # ì—…ë¡œë“œí•  ì „ì²´ ì œëª© ëª©ë¡\n",
    "                contents = upload_contents, # ì—…ë¡œë“œí•  ì „ì²´ ë‚´ìš© ëª©ë¡\n",
    "                # uploading_day ë¯¸ ì§€ì • ì‹œ ë‚´ì¼ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì—…ë¡œë“œ ,\n",
    "                uploading_start_hour = 2, # ì—…ë¡œë“œ ì‹œì‘ ì‹œê°„ (ê¸°ë³¸ê°’ 2ì‹œ)\n",
    "                uploading_minute_term = 120)  # ì—…ë¡œë“œ ê°„ê²© (ê¸°ë³¸ê°’ 120ë¶„)\n",
    "        finally :\n",
    "            self.uploader.driver.quit() # ë¦¬ì†ŒìŠ¤ ì ˆì•½ì„ ìœ„í•œ ë“œë¼ì´ë²„ ì¢…ë£Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê¸°ë³¸ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’›\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "True\n",
      "True\n",
      "Data loaded from JSON\n",
      "ğŸ’›\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "True\n",
      "True\n",
      "Data loaded from JSON\n",
      "ğŸ’›\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "True\n",
      "True\n",
      "Data loaded from JSON\n",
      "ğŸ’›\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "True\n",
      "True\n",
      "Data loaded from JSON\n",
      "ğŸ’›\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ê¸°ì¡´ì— ìƒì„±ëœ í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "True\n",
      "True\n",
      "Data loaded from JSON\n"
     ]
    }
   ],
   "source": [
    "# ê²½ê³ ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ì €ì¥ (.env) í™•ì¸\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "isHeadless = True\n",
    "verbose = True\n",
    "\n",
    "# blogname = input('ë¸”ë¡œê·¸ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” ')\n",
    "print('ğŸ’›')\n",
    "statifi = Blogger(blogname = 'statifi', verbose = verbose, isHeadless = isHeadless)\n",
    "print('ğŸ’›')\n",
    "kindmom = Blogger(blogname = 'kindmom', verbose = verbose, isHeadless = isHeadless)\n",
    "print('ğŸ’›')\n",
    "sweetkiwi = Blogger(blogname = 'sweetkiwi', verbose = verbose, isHeadless = isHeadless)\n",
    "print('ğŸ’›')\n",
    "takuz = Blogger(blogname = 'takuz', verbose = verbose, isHeadless = isHeadless)\n",
    "print('ğŸ’›')\n",
    "morningbbobbo = Blogger(blogname = 'morningbbobbo', verbose = verbose, isHeadless = isHeadless)\n",
    "\n",
    "bloggers ={\n",
    "    \"statifi\" :statifi,\n",
    "    \"kindmom\" :kindmom,\n",
    "    \"sweetkiwi\" :sweetkiwi,\n",
    "    \"takuz\" :takuz,\n",
    "    \"morningbbobbo\" :morningbbobbo,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [data, id]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [data, id]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [data, id]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [data, id]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [data, id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”ì‹œ ë°˜ë³µë¬¸\n",
    "\n",
    "\n",
    "for k, v in bloggers.items() :\n",
    "    print(v.posting_ai.vectorstore_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í´ë¦­1 : í‚¤ì›Œë“œ ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended = {\n",
    "    \"ê±´ê°•ê³¼ ì›°ë¹™\": [\"ìš”ê°€\", \"ë§ˆì¸ë“œí’€ë‹ˆìŠ¤\", \"ì›°ë‹ˆìŠ¤ ë¦¬íŠ¸ë¦¿\", \"ê±´ê°•í•œ ê°„ì‹\", \"ëª…ìƒ\", \"í•„ë¼í…ŒìŠ¤\", \"ê±´ê°•í•œ ì‹ë‹¨\", \"ê±´ê°• ì¶”ì„¸\", \"íë§\", \"ê±´ê°• ê²€ì§„\"],\n",
    "    \"ê¸°ì—…ê³¼ ê¸°ì—…ê°€\": [\"ê¸°ì—…ê°€ ì •ì‹ \", \"í¬ë¼ìš°ë“œí€ë”©\", \"ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ í˜ì‹ \", \"ê¸°ìˆ  ë²¤ì²˜\", \"ë²¤ì²˜ íˆ¬ì\", \"ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„\"],\n",
    "    \"ì¬íƒ ê·¼ë¬´ ë° ìê¸° ê³„ë°œ ë¸”ë¡œê·¸\": [\"ì¬íƒ ê·¼ë¬´ íŒ\", \"ìê¸°ê³„ë°œ ì±… ì¶”ì²œ\", \"ì˜¨ë¼ì¸ í•™ìŠµ í”Œë«í¼\", \"ì‹œê°„ ê´€ë¦¬ ê¸°ìˆ \", \"ìƒì‚°ì„± í–¥ìƒ\", \"ì‚¬ì´ë“œ í—ˆìŠ¬\", \"ì¬íƒ ê·¼ë¬´ í™˜ê²½\", \"í”„ë¦¬ëœì„œ ê°€ì´ë“œ\", \"ìê¸° ê´€ë¦¬\", \"ëª¨í‹°ë² ì´ì…˜ ì¦ì§„\"],\n",
    "    \"ê±´ê°• ë° ì›°ë‹ˆìŠ¤ ë¸”ë¡œê·¸\": [\"ê±´ê°•í•œ ì‹ë‹¨\", \"ì •ì‹  ê±´ê°•\", \"ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬\", \"í™ˆ í”¼íŠ¸ë‹ˆìŠ¤\", \"ëª…ìƒ ê¸°ë²•\", \"ì›°ë‹ˆìŠ¤ ë¼ì´í”„ìŠ¤íƒ€ì¼\", \"ì˜ì–‘ ì •ë³´\", \"ê±´ê°• ê²€ì§„ ê°€ì´ë“œ\", \"ì²´ì¤‘ ê´€ë¦¬\", \"ê±´ê°•í•œ ìŠµê´€\"],\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# ê°€ì • ê´€ë¦¬ ë° DIY í”„ë¡œì íŠ¸ ë¸”ë¡œê·¸\n",
    "subjects_n_words_for_statifi = {\"ê°€ì • ê´€ë¦¬ ë° DIY í”„ë¡œì íŠ¸ ë¸”ë¡œê·¸\": [\"í™ˆ ë°ì½” ì•„ì´ë””ì–´\", \"ìë™ì°¨ ê´€ë¦¬\", \"ì •ì› ê°€ê¾¸ê¸°\", \"DIY í”„ë¡œì íŠ¸\", \"ê°€ì • ìœ ì§€ë³´ìˆ˜\", \"ê°€êµ¬ ì œì‘\", \"ì§‘ì•ˆ ì •ë¦¬ íŒ\", \"ì—ë„ˆì§€ íš¨ìœ¨ ê°€ì´ë“œ\", \"ì•ˆì „í•œ ì§‘ ë§Œë“¤ê¸°\", \"ìœ ì•„ êµìœ¡ í™œë™\"] }\n",
    "# ìŒì‹ ìš”ë¦¬ \n",
    "subjects_n_words_for_kindmom = {\"ìŒì‹ê³¼ ìš”ë¦¬\": ['ë¯¸ì‰ë¦°', 'ë¯¸ìŠë­', 'ë¸”ë£¨ë¦¬ë³¸', '100ë…„ê°€ê²Œ', \"ë² ì§€í…Œë¦¬ì–¸ ë ˆì‹œí”¼\", \"ìŒì‹ íŠ¸ë Œë“œ\", \"ìŒì‹ ì‹œì¥\", \"ë¡œì»¬ ì‹ë‹¹ íƒë°©\", \"ë””í†¡ìŠ¤ ìš”ë¦¬\", \"ìˆ˜í¼í‘¸ë“œ\", \"í–¥í†  ìŒì‹\", \"ìŒì‹ ë¸”ë¡œê·¸\", \"ì¿ í‚¹ í´ë˜ìŠ¤\", \"í‘¸ë“œ íˆ¬ì–´\"], }\n",
    "# ì—¬í–‰\n",
    "subjects_n_words_for_sweetkiwi = {\"ë””ì§€í„¸ ë…¸ë§ˆë“œì™€ ì—¬í–‰ ë¸”ë¡œê·¸\": [\"ë””ì§€í„¸ ë…¸ë§ˆë“œ íŒ\", \"ê¸€ë¡œë²Œ ì—¬í–‰ ê°€ì´ë“œ\", \"ì‘ì—… íš¨ìœ¨ì„±\", \"ì—¬í–‰ìš© ì•±\", \"ì—¬í–‰ ì˜ˆì‚° ê´€ë¦¬\", \"ë¹„ì ê°€ì´ë“œ\", \"ì—¬í–‰ì ë³´í—˜\", \"ì‘ì—… ê³µê°„ ì°¾ê¸°\", \"ë¬¸í™” ì ì‘ íŒ\", \"ì†Œì…œ ë¯¸ë””ì–´ ë§ˆì¼€íŒ…\"],  }\n",
    "# ì¸ê³µì§€ëŠ¥, \n",
    "subjects_n_words_for_takuz = {\"ì¸ê³µì§€ëŠ¥\": [\"ì¸ê³µì§€ëŠ¥ ì‘ìš©\", \"ë¨¸ì‹  ëŸ¬ë‹\", \"ë°ì´í„° ë¶„ì„\", \"ë”¥ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜\", \"í•€í…Œí¬\", \"ì—ì½”í…Œí¬\", \"ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ\", \"í…Œí¬ ë¦¬ë·°\"]}\n",
    "# ì›°ë‹ˆìŠ¤\n",
    "subjects_n_words_for_morningbbobbo = {\"ì§€ì† ê°€ëŠ¥í•œ ìƒí™œ ë¸”ë¡œê·¸\": [\"ì œë¡œ ì›¨ì´ìŠ¤íŠ¸ íŒ\", \"ì§€ì† ê°€ëŠ¥í•œ ì œí’ˆ\", \"ì¹œí™˜ê²½ ìƒí™œ\", \"ì—ì½” ë¼ì´í”„ìŠ¤íƒ€ì¼\", \"í™˜ê²½ ë³´í˜¸\", \"í”Œë¼ìŠ¤í‹± í”„ë¦¬\", \"ì¬ì‚¬ìš© ê°€ëŠ¥ ì œí’ˆ\", \"ì¹œí™˜ê²½ í™ˆ ë©”ì´í¬ì˜¤ë²„\", \"ì§€ì† ê°€ëŠ¥í•œ íŒ¨ì…˜\", \"ì—ë„ˆì§€ ì ˆì•½ íŒ\"], }\n",
    "\n",
    "\n",
    "\n",
    "depth = 3\n",
    "save = True\n",
    "\n",
    "\n",
    "import threading\n",
    "\n",
    "def collect_keywords_for_blogger(blogger, subjects_n_words, depth, save):\n",
    "    print(f\"Collecting keywords for {blogger.blogname}...\")\n",
    "    blogger.collect_keywords(subjects_n_words=subjects_n_words, depth=depth, save=save)\n",
    "    print(f\"Finished collecting keywords for {blogger.blogname}.\")\n",
    "\n",
    "# ê° ë¸”ë¡œê±°ì˜ ì£¼ì œ ë° í‚¤ì›Œë“œ ë°ì´í„°\n",
    "bloggers = {\n",
    "    'statifi': statifi,\n",
    "    'kindmom': kindmom,\n",
    "    'sweetkiwi': sweetkiwi,\n",
    "    'takuz': takuz,\n",
    "    'morningbbobbo': morningbbobbo\n",
    "}\n",
    "\n",
    "subjects_n_words = {\n",
    "    'statifi': subjects_n_words_for_statifi,\n",
    "    'kindmom': subjects_n_words_for_kindmom,\n",
    "    'sweetkiwi': subjects_n_words_for_sweetkiwi,\n",
    "    'takuz': subjects_n_words_for_takuz,\n",
    "    'morningbbobbo': subjects_n_words_for_morningbbobbo\n",
    "}\n",
    "\n",
    "# ìŠ¤ë ˆë“œ ëª©ë¡ ìƒì„±\n",
    "threads = []\n",
    "\n",
    "# ê° ë¸”ë¡œê±°ì— ëŒ€í•´ ìŠ¤ë ˆë“œ ìƒì„± ë° ì‹¤í–‰\n",
    "for blogger_name, blogger in bloggers.items():\n",
    "    thread = threading.Thread(target=collect_keywords_for_blogger, args=(blogger, subjects_n_words[blogger_name], depth, save))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# ëª¨ë“  ìŠ¤ë ˆë“œì˜ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¼\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "print(\"All keyword collection tasks completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í´ë¦­ 2 : í¬ìŠ¤íŒ… ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_contents_creation = input('ìƒì„±í•  ì»¨í…ì¸  ê°¯ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ë¬´í•œëŒ€ìƒì‚°ì€ ì—”í„°ë¥¼ ì¹˜ì„¸ìš” ')\n",
    "# if num_contents_creation is not None :\n",
    "#     num_contents_creation = int(num_contents_creation)\n",
    "\n",
    "\n",
    "# kindmom.create_contents(num_contents_creation)\n",
    "# takuz.create_contents(num_contents_creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ£statifi : kindmom : \n",
      "\n",
      "ğŸŸ£sweetkiwi : \n",
      "ğŸŸ£takuz : \n",
      "ğŸŸ£morningbbobbo : \n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "ë¬¸í™”ì¬ì²­ ciğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Cultural Heritage Administration CIâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "ë¬¸í™”ì¬ì²­ ë°©ë¬¸ìì—¬ê¶ŒğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Cultural Heritage Administration Visitor's Passportâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "ë¬¸í™”ì¬ì²­ ë°©ë¬¸ì ì—¬ê¶ŒğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Cultural Heritage Administration Visitor Passportâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "contents saved as JSON :  c:\\hellodear\\tistory_ebook\\kindmom\\storage\\[[kindmom]]_((ko))_``ë¬¸í™”ì¬ì²­ ci ë° ë°©ë¬¸ìì—¬ê¶Œ ì•ˆë‚´ ë¬¸í™”ì¬ì²­ ë°©ë¬¸ ì‹œ í•„ìˆ˜ì‚¬í•­ë“¤!``.json\n",
      "Data successfully saved to c:\\hellodear\\tistory_ebook\\kindmom\\for_upload\\[[kindmom]]_((ko))_``ë¬¸í™”ì¬ì²­ ci ë° ë°©ë¬¸ìì—¬ê¶Œ ì•ˆë‚´ ë¬¸í™”ì¬ì²­ ë°©ë¬¸ ì‹œ í•„ìˆ˜ì‚¬í•­ë“¤!``.txt\n",
      "ì—¬í–‰ì˜ˆì‚° ì—‘ì…€ğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Travel Budget ExcelâŒ ì´ë¯¸ì§€ìˆ˜ì§‘ ì‹¤íŒ¨(0)\n",
      "ì—¬í–‰ê³„íš ì—‘ì…€ğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Plan your trip Excelâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "ì—¬í–‰ ì˜ˆì‚° ì—‘ì…€ğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Travel Budget ExcelâŒ ì´ë¯¸ì§€ìˆ˜ì§‘ ì‹¤íŒ¨(0)\n",
      "contents saved as JSON :  c:\\hellodear\\tistory_ebook\\sweetkiwi\\storage\\[[sweetkiwi]]_((ko))_``ì—¬í–‰ì˜ˆì‚° ì—‘ì…€ ë° ì—¬í–‰ê³„íš ì—‘ì…€ í…œí”Œë¦¿ ì†Œê°œ!``.json\n",
      "Data successfully saved to c:\\hellodear\\tistory_ebook\\sweetkiwi\\for_upload\\[[sweetkiwi]]_((ko))_``ì—¬í–‰ì˜ˆì‚° ì—‘ì…€ ë° ì—¬í–‰ê³„íš ì—‘ì…€ í…œí”Œë¦¿ ì†Œê°œ!``.txt\n",
      "ê°€ëŠ¥ì„± ë†’ì€ğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Likelyâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "ì‹ ë¢°í• ë§Œí•œğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Reliableâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "í™•ë¥  ë†’ì€ğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Probableâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "contents saved as JSON :  c:\\hellodear\\tistory_ebook\\takuz\\storage\\[[takuz]]_((ko))_``ê°€ëŠ¥ì„± ë†’ì€ ì‹ ë¢°í• ë§Œí•œ í™•ë¥  ë†’ì€ ë°©ë²•ìœ¼ë¡œ ì„±ê³µì„ ê°€ì¥ likelyí•˜ê²Œ ë§Œë“œëŠ” ë°©ë²•!``.json\n",
      "Data successfully saved to c:\\hellodear\\tistory_ebook\\takuz\\for_upload\\[[takuz]]_((ko))_``ê°€ëŠ¥ì„± ë†’ì€ ì‹ ë¢°í• ë§Œí•œ í™•ë¥  ë†’ì€ ë°©ë²•ìœ¼ë¡œ ì„±ê³µì„ ê°€ì¥ likelyí•˜ê²Œ ë§Œë“œëŠ” ë°©ë²•!``.txt\n",
      "í™ˆí…Œí¬ ê¸°ê¸°ğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Hometech Devicesâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "í™ˆí…Œí¬ ì‹œì¥ ë™í–¥ğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Home Tech Market Trendsâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "í™ˆí…Œí¬ ì œí’ˆ ì¶”ì²œğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Hometech Product Recommendationsâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "contents saved as JSON :  c:\\hellodear\\tistory_ebook\\morningbbobbo\\storage\\[[morningbbobbo]]_((ko))_``í™ˆí…Œí¬ ê¸°ê¸°, ì‹œì¥ ë™í–¥, ì œí’ˆ ì¶”ì²œ - ì „ë¬¸ê°€ê°€ ì•Œë ¤ì£¼ëŠ” ìµœì‹  í™ˆí…Œí¬ íŠ¸ë Œë“œ!``.json\n",
      "Data successfully saved to c:\\hellodear\\tistory_ebook\\morningbbobbo\\for_upload\\[[morningbbobbo]]_((ko))_``í™ˆí…Œí¬ ê¸°ê¸°, ì‹œì¥ ë™í–¥, ì œí’ˆ ì¶”ì²œ - ì „ë¬¸ê°€ê°€ ì•Œë ¤ì£¼ëŠ” ìµœì‹  í™ˆí…Œí¬ íŠ¸ë Œë“œ!``.txt\n",
      "ì‡ì½”ë‹¤í…Œ ì—¬í–‰ğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Ikkodate Tourismâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "ì‡ì½”ë‹¤í…Œ ë§›ì§‘ğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Ikkodate Restaurantsâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "ì‡ì½”ë‹¤í…Œ í˜¸í…”ğŸ’¬ ì´ë¯¸ì§€ìˆ˜ì§‘\n",
      "Ikkodate Hotelsâœ… ì´ë¯¸ì§€ìˆ˜ì§‘ (1)\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "contents saved as JSON :  c:\\hellodear\\tistory_ebook\\statifi\\storage\\[[statifi]]_((ko))_``ì‡ì½”ë‹¤í…Œ ì—¬í–‰, ë§›ì§‘, í˜¸í…” ì¶”ì²œ - ì‡ì½”ë‹¤í…Œì—ì„œ ì¦ê¸°ëŠ” ì™„ë²½í•œ íœ´ê°€ ì•ˆë‚´``.json\n",
      "Data successfully saved to c:\\hellodear\\tistory_ebook\\statifi\\for_upload\\[[statifi]]_((ko))_``ì‡ì½”ë‹¤í…Œ ì—¬í–‰, ë§›ì§‘, í˜¸í…” ì¶”ì²œ - ì‡ì½”ë‹¤í…Œì—ì„œ ì¦ê¸°ëŠ” ì™„ë²½í•œ íœ´ê°€ ì•ˆë‚´``.txt\n",
      "'contents_bef_trans'\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "'contents_bef_trans'\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "'contents_bef_trans'\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "'contents_bef_trans'\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "'contents_bef_trans'\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "'contents_bef_trans'\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "'contents_bef_trans'\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "'contents_bef_trans'\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "âœ… ë²¡í† ì–´ìŠ¤í† ì–´ ë‚´ ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ\n",
      "'contents_bef_trans'\n",
      "'contents_bef_trans'\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "\n",
    "\n",
    "def create_statifi_contents():\n",
    "    print('ğŸŸ£statifi : ')\n",
    "    statifi.create_contents()\n",
    "def create_kindmom_contents():\n",
    "    print('kindmom : ')\n",
    "    kindmom.create_contents()\n",
    "def create_sweetkiwi_contents():\n",
    "    print('ğŸŸ£sweetkiwi : ')\n",
    "    sweetkiwi.create_contents()    \n",
    "def create_takuz_contents():\n",
    "    print('ğŸŸ£takuz : ')\n",
    "    takuz.create_contents()    \n",
    "def create_morningbbobbo_contents():\n",
    "    print('ğŸŸ£morningbbobbo : ')\n",
    "    morningbbobbo.create_contents()\n",
    "\n",
    "# ê°ê°ì˜ ìŠ¤ë ˆë“œ ìƒì„±\n",
    "thread_statifi = threading.Thread(target=create_statifi_contents)\n",
    "thread_kindmom = threading.Thread(target=create_kindmom_contents)\n",
    "thread_sweetkiwi = threading.Thread(target=create_sweetkiwi_contents)\n",
    "thread_takuz = threading.Thread(target=create_takuz_contents)\n",
    "thread_morningbbobbo = threading.Thread(target=create_morningbbobbo_contents)\n",
    "\n",
    "# ìŠ¤ë ˆë“œ ì‹¤í–‰\n",
    "thread_statifi.start()\n",
    "thread_kindmom.start()\n",
    "thread_sweetkiwi.start()\n",
    "thread_takuz.start()\n",
    "thread_morningbbobbo.start()\n",
    "\n",
    "# ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°\n",
    "thread_statifi.join()\n",
    "thread_kindmom.join()\n",
    "thread_sweetkiwi.join()\n",
    "thread_takuz.join()\n",
    "thread_morningbbobbo.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í´ë¦­3 : í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_contents_upload = int(input('ì—…ë¡œë“œ í•  ì»¨í…ì¸  ê°¯ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ë¬´í•œëŒ€ìƒì‚°ì€ ì—”í„°ë¥¼ ì¹˜ì„¸ìš”. '))\n",
    "\n",
    "assert num_contents_upload is not None, 'ì—…ë¡œë“œí•  ì»¨í…ì¸  ê°¯ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.'\n",
    "assert num_contents_upload <= 15 , '15ê°œ ì´í•˜ë¡œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.'\n",
    "\n",
    "blogger.upload_contents(num_contents_upload)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
